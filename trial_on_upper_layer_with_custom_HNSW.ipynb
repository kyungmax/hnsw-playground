{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Upper layer의 long bridge 조작 - trial1 w/ custom HNSW"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T02:00:01.319082Z",
     "start_time": "2025-10-31T02:00:01.020585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import struct\n",
    "import networkx as nx\n",
    "from heapq import heapify, heappop, heappush, heapreplace, nlargest, nsmallest\n",
    "from operator import itemgetter\n",
    "from random import random\n",
    "\n",
    "from numpy.f2py.auxfuncs import throw_error\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "\n",
    "class HNSW:\n",
    "    # self._graphs[level][i] contains a {j: dist} dictionary,\n",
    "    # where j is a neighbor of i and dist is distance\n",
    "\n",
    "   # L2 / cosine (스칼라) 함수는 기존대로 유지하되 안정성 보강\n",
    "    def l2_distance(self, a, b):\n",
    "        return float(np.linalg.norm(a - b))\n",
    "\n",
    "    def cosine_distance(self, a, b):\n",
    "        na = np.linalg.norm(a) + 1e-12\n",
    "        nb = np.linalg.norm(b) + 1e-12\n",
    "        return 1.0 - float(np.dot(a, b) / (na * nb))\n",
    "\n",
    "    def vectorized_distance_(self, x, ys):\n",
    "        ys_arr = np.asarray(ys)\n",
    "        if ys_arr.ndim == 1:\n",
    "            ys_arr = ys_arr.reshape(1, -1)\n",
    "        if self.distance_type == \"l2\":\n",
    "            # squared or actual norm: 기존 스칼라가 np.linalg.norm을 사용하므로 일관성 위해 norm 사용\n",
    "            return np.linalg.norm(ys_arr - x, axis=1)\n",
    "        elif self.distance_type == \"cosine\":\n",
    "            x_norm = x / (np.linalg.norm(x) + 1e-12)\n",
    "            ys_norm = ys_arr / (np.linalg.norm(ys_arr, axis=1, keepdims=True) + 1e-12)\n",
    "            return 1.0 - (ys_norm @ x_norm)\n",
    "        else:\n",
    "            # fallback: 호출 가능한 distance_func으로 루프\n",
    "            return np.array([self.distance_func(x, y) for y in ys_arr])\n",
    "\n",
    "    def __init__(self, distance_type, M=5, efConstruction=200, Mmax=None):\n",
    "        if distance_type == \"l2\":\n",
    "            distance_func = self.l2_distance\n",
    "        elif distance_type == \"cosine\":\n",
    "            distance_func = self.cosine_distance\n",
    "        else:\n",
    "            raise TypeError('Please check your distance type!')\n",
    "        self.distance_func = distance_func\n",
    "        self.distance_type = distance_type  # 추가\n",
    "        self.vectorized_distance = self.vectorized_distance_\n",
    "        self._M = M\n",
    "        self._efConstruction = efConstruction\n",
    "        self._Mmax = 2 * M if Mmax is None else Mmax\n",
    "        self._level_mult = 1 / np.log(M)\n",
    "        self._graphs = []\n",
    "        self._enter_point = None\n",
    "        self.data = []\n",
    "        self.visited_count = 0\n",
    "\n",
    "        ##########\n",
    "        self.visited_per_hop = []\n",
    "        self.ann_per_hop = []\n",
    "        #########\n",
    "\n",
    "    ### Algorithm 1: INSERT\n",
    "    def insert(self, q, efConstruction=None):\n",
    "\n",
    "        if efConstruction is None:\n",
    "            efConstruction = self._efConstruction\n",
    "\n",
    "        distance = self.distance_func\n",
    "        data = self.data\n",
    "        graphs = self._graphs\n",
    "        ep = self._enter_point\n",
    "        M = self._M\n",
    "\n",
    "        # line 4: determine level for the new element q\n",
    "        l = int(-np.log(random()) * self._level_mult) + 1\n",
    "        idx = len(data)\n",
    "        data.append(q)\n",
    "\n",
    "        if ep is not None:\n",
    "            neg_dist = -distance(q, data[ep])\n",
    "            # distance(q, data[ep])|\n",
    "\n",
    "            # line 5-7: find the closest neighbor for levels above the insertion level\n",
    "            for lc in reversed(graphs[l:]):\n",
    "                neg_dist, ep = self._search_layer(q, [(neg_dist, ep)], lc, 1)[0]\n",
    "\n",
    "            # line 8-17: insert q at the relevant levels; W is a candidate list\n",
    "            layer0 = graphs[0]\n",
    "            W = [(neg_dist, ep)]  ## 추가\n",
    "\n",
    "            for lc in reversed(graphs[:l]):\n",
    "                M_layer = M if lc is not layer0 else self._Mmax\n",
    "\n",
    "                # line 9: update W with the closest nodes found in the graph\n",
    "                W = self._search_layer(q, W, lc, efConstruction)  ## 변경\n",
    "\n",
    "                # line 10: insert the best neighbors for q at this layer\n",
    "                lc[idx] = layer_idx = {}\n",
    "                self._select(layer_idx, W, M_layer, lc, heap=True)\n",
    "\n",
    "                # line 11-13: insert bidirectional links to the new node\n",
    "                for j, dist in layer_idx.items():\n",
    "                    self._select(lc[j], (idx, dist), M_layer, lc)\n",
    "\n",
    "        # line 18: create empty graphs for all new levels\n",
    "        for _ in range(len(graphs), l):\n",
    "            graphs.append({idx: {}})\n",
    "            self._enter_point = idx\n",
    "\n",
    "    ### Algorithm 5: K-NN-SEARCH\n",
    "    def search(self, q, K=5, efSearch=20):\n",
    "        \"\"\"Find the K points closest to q.\"\"\"\n",
    "\n",
    "        distance = self.distance_func\n",
    "        graphs = self._graphs\n",
    "        ep = self._enter_point\n",
    "        self.visited_count = 0\n",
    "\n",
    "        if ep is None:\n",
    "            raise ValueError(\"Empty graph\")\n",
    "\n",
    "        neg_dist = -distance(q, self.data[ep])\n",
    "\n",
    "        # line 1-5: search from top layers down to the second level\n",
    "        for lc in reversed(graphs[1:]):\n",
    "            neg_dist, ep = self._search_layer(q, [(neg_dist, ep)], lc, 1)[0]\n",
    "\n",
    "        ##########\n",
    "        self.visited_per_hop = []\n",
    "        self.ann_per_hop = []\n",
    "        ##########\n",
    "\n",
    "        # line 6: search with efSearch neighbors at the bottom level\n",
    "        W = self._search_layer(q, [(neg_dist, ep)], graphs[0], efSearch)\n",
    "\n",
    "        if K is not None:\n",
    "            W = nlargest(K, W)\n",
    "        else:\n",
    "            W.sort(reverse=True)\n",
    "\n",
    "        return [(idx, -md) for md, idx in W]\n",
    "\n",
    "    ### Algorithm 2: SEARCH-LAYER\n",
    "    def _search_layer(self, q, W, lc, ef):\n",
    "\n",
    "        vectorized_distance = self.vectorized_distance\n",
    "        data = self.data\n",
    "\n",
    "        # Step 1: Initialize candidate list and visited set\n",
    "        C = [(-neg_dist, idx) for neg_dist, idx in W]\n",
    "        heapify(C)\n",
    "        heapify(W)\n",
    "        visited = set(idx for _, idx in W)\n",
    "\n",
    "        # Step 4-17: Explore neighbors until candidate list is exhausted\n",
    "        while C:\n",
    "            dist, c = heappop(C)\n",
    "            furthest = -W[0][0]\n",
    "            if dist > furthest:\n",
    "                break\n",
    "            neighbors = [e for e in lc[c] if e not in visited]\n",
    "            visited.update(neighbors)\n",
    "            if neighbors:\n",
    "                # data에서 한 번에 슬라이스하여 배열 생성 후 벡터화 계산\n",
    "                ys = np.vstack([data[e] for e in neighbors])\n",
    "                dists = vectorized_distance(q, ys)\n",
    "                for e, dist in zip(neighbors, dists):\n",
    "                    self.visited_count += 1\n",
    "                    neg_dist = -float(dist)\n",
    "                    if len(W) < ef:\n",
    "                        heappush(C, (float(dist), e))\n",
    "                        heappush(W, (neg_dist, e))\n",
    "                        furthest = -W[0][0]\n",
    "                    elif dist < furthest:\n",
    "                        heappush(C, (float(dist), e))\n",
    "                        heapreplace(W, (neg_dist, e))\n",
    "                        furthest = -W[0][0]\n",
    "\n",
    "            ##########\n",
    "            self.visited_per_hop.append(len(visited))\n",
    "            topk = nsmallest(min(ef, len(W)), ((-neg, idx) for neg, idx in W))  # (dist, id)\n",
    "            self.ann_per_hop.append([idx for _, idx in topk])\n",
    "            ##########\n",
    "\n",
    "        return W\n",
    "\n",
    "    ### Algorithm 3: SELECT-NEIGHBORS-SIMPLE\n",
    "    def _select(self, R, C, M, lc, heap=False):\n",
    "\n",
    "        if not heap:\n",
    "            idx, dist = C\n",
    "            if len(R) < M:\n",
    "                R[idx] = dist\n",
    "            else:\n",
    "                max_idx, max_dist = max(R.items(), key=itemgetter(1))\n",
    "                if dist < max_dist:\n",
    "                    del R[max_idx]\n",
    "                    R[idx] = dist\n",
    "            return\n",
    "\n",
    "        else:\n",
    "            C = nlargest(M, C)\n",
    "            R.update({idx: -neg_dist for neg_dist, idx in C})\n",
    "\n",
    "\n",
    "    # ======== Upper-layer graph editing helpers ========\n",
    "    def max_level_map(self):\n",
    "        \"\"\"Return dict: node -> highest level it appears in.\"\"\"\n",
    "        lvl = defaultdict(int)\n",
    "        for li, lc in enumerate(self._graphs):\n",
    "            for i in lc.keys():\n",
    "                if i not in lvl or li > lvl[i]:\n",
    "                    lvl[i] = li\n",
    "        return dict(lvl)\n",
    "\n",
    "    def node_in_level(self, level, idx):\n",
    "        return idx in self._graphs[level]\n",
    "\n",
    "    def ensure_node_in_level(self, level, idx):\n",
    "        if idx not in self._graphs[level]:\n",
    "            self._graphs[level][idx] = {}\n",
    "\n",
    "    def add_undirected_edge(self, level, u, v):\n",
    "        \"\"\"Add bidirectional edge (u,v) at given level using current distance func and M cap.\"\"\"\n",
    "        lc = self._graphs[level]\n",
    "        # ensure nodes exist\n",
    "        if u not in lc:\n",
    "            lc[u] = {}\n",
    "        if v not in lc:\n",
    "            lc[v] = {}\n",
    "        d = self.distance_func(self.data[u], self.data[v])\n",
    "        # add to u\n",
    "        self._select(lc[u], (v, d), self._M, lc, heap=False)\n",
    "        # add to v\n",
    "        self._select(lc[v], (u, d), self._M, lc, heap=False)\n",
    "\n",
    "    def drop_undirected_edge(self, level, u, v):\n",
    "        lc = self._graphs[level]\n",
    "        if u in lc and v in lc[u]:\n",
    "            del lc[u][v]\n",
    "        if v in lc and u in lc[v]:\n",
    "            del lc[v][u]\n",
    "\n",
    "    # ======== Upper-layer robust swap helpers (M 제한을 반드시 지킬수 있도록) ========\n",
    "    def _distance_idx(self, i, j):\n",
    "        return self.distance_func(self.data[i], self.data[j])\n",
    "\n",
    "    def _prune_node_to_M(self, level, u, candidate_ids):\n",
    "        \"\"\"Rebuild u's neighbor list at given level by pruning the candidate set down to M_layer using _select().\n",
    "        Returns the set of kept neighbor ids. \"\"\"\n",
    "        lc = self._graphs[level]\n",
    "        # ensure node exists\n",
    "        if u not in lc:\n",
    "            lc[u] = {}\n",
    "        # decide cap per level\n",
    "        M_layer = self._M if level != 0 else self._Mmax\n",
    "        # build C as heap items (neg_dist, idx)\n",
    "        C = []\n",
    "        for v in set(candidate_ids):\n",
    "            if v == u:\n",
    "                continue\n",
    "            d = self._distance_idx(u, v)\n",
    "            C.append((-d, v))\n",
    "        # prune\n",
    "        R = {}\n",
    "        self._select(R, C, M_layer, lc, heap=True)\n",
    "        lc[u] = dict(R)\n",
    "        return set(lc[u].keys())\n",
    "\n",
    "    def apply_swaps_with_cap(self, level, drops, adds):\n",
    "        \"\"\"Apply batched drops/adds at a level while enforcing degree cap M per node and reciprocity.\n",
    "        - drops: list of (u, v)\n",
    "        - adds : list of (u, v)\n",
    "        \"\"\"\n",
    "        lc = self._graphs[level]\n",
    "        # build per-node to_drop/to_add\n",
    "        to_drop = defaultdict(set)\n",
    "        to_add  = defaultdict(set)\n",
    "        for (u, v) in drops:\n",
    "            to_drop[u].add(v)\n",
    "            to_drop[v].add(u)\n",
    "        for (u, v) in adds:\n",
    "            to_add[u].add(v)\n",
    "            to_add[v].add(u)\n",
    "        # compute pools and prune per node\n",
    "        affected = set(list(to_drop.keys()) + list(to_add.keys()))\n",
    "        # ensure nodes exist\n",
    "        for u in affected:\n",
    "            if u not in lc:\n",
    "                lc[u] = {}\n",
    "        # gather pools\n",
    "        new_neighbors = {}\n",
    "        for u in affected:\n",
    "            cur = set(lc.get(u, {}).keys())\n",
    "            pool = (cur - to_drop[u]) | to_add[u]\n",
    "            kept = self._prune_node_to_M(level, u, pool)\n",
    "            new_neighbors[u] = kept\n",
    "        # reciprocity fix: ensure symmetry\n",
    "        for u in affected:\n",
    "            for v in list(new_neighbors[u]):\n",
    "                if u not in self._graphs[level].get(v, {}):\n",
    "                    # add back with distance\n",
    "                    d = self._distance_idx(u, v)\n",
    "                    self._select(self._graphs[level][v], (u, d), self._M if level != 0 else self._Mmax, self._graphs[level], heap=False)\n",
    "            # remove broken reciprocals\n",
    "            curv = set(self._graphs[level][u].keys())\n",
    "            for v in list(curv):\n",
    "                if u not in self._graphs[level].get(v, {}):\n",
    "                    # remove u from v if needed\n",
    "                    if v in self._graphs[level] and u in self._graphs[level][v]:\n",
    "                        del self._graphs[level][v][u]"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Preparation"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T02:00:06.831608Z",
     "start_time": "2025-10-31T02:00:02.506902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_fvecs(filename):\n",
    "    \"\"\"Reads .fvecs binary file into np.ndarray of shape (n, d).\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = f.read()\n",
    "    dim = struct.unpack('i', data[:4])[0]\n",
    "    vecs = np.frombuffer(data, dtype=np.float32)\n",
    "    vecs = vecs.reshape(-1, dim + 1)[:, 1:]  # drop the leading 'dim'\n",
    "    return vecs\n",
    "\n",
    "def read_ivecs(filename):\n",
    "    \"\"\"Reads .ivecs binary file into np.ndarray of shape (n, k).\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = f.read()\n",
    "    dim = struct.unpack('i', data[:4])[0]\n",
    "    vecs = np.frombuffer(data, dtype=np.int32)\n",
    "    vecs = vecs.reshape(-1, dim + 1)[:, 1:]\n",
    "    return vecs\n",
    "\n",
    "def compute_true_neighbors(data, query, k, distance_func):\n",
    "    \"\"\"Compute true k-nearest neighbors for query vectors from data.\"\"\"\n",
    "    true_neighbors = []\n",
    "    for q in query:\n",
    "        dists = np.array([distance_func(q, x) for x in data])\n",
    "        nn_indices = np.argsort(dists)[:k]\n",
    "        true_neighbors.append(nn_indices)\n",
    "    return np.array(true_neighbors)\n",
    "\n",
    "def exact_topk(train_subset, queries, K, distance_type='l2'):\n",
    "    out = np.empty((len(queries), K), dtype=np.int32)\n",
    "    for i, q in enumerate(queries):\n",
    "        if distance_type == 'l2':\n",
    "            d = np.sum((train_subset - q) ** 2, axis=1)\n",
    "        elif distance_type == 'angular' or distance_type == 'cosine':\n",
    "            # L2 정규화 필요\n",
    "            q_norm = q / (np.linalg.norm(q) + 1e-12)\n",
    "            train_norm = train_subset / (np.linalg.norm(train_subset, axis=1, keepdims=True) + 1e-12)\n",
    "            d = 1.0 - np.dot(train_norm, q_norm)\n",
    "        else:\n",
    "            raise ValueError(\"distance_type은 'l2' 또는 'angular'이어야 합니다.\")\n",
    "        idx = np.argpartition(d, K)[:K]\n",
    "        idx = idx[np.argsort(d[idx])]\n",
    "        out[i] = idx\n",
    "    return out\n",
    "# 데이터셋 경로 (현재 구조에 맞춰 수정)\n",
    "base_path = \"./datasets\"\n",
    "# 데이터셋 경로\n",
    "file_path = base_path + \"/glove-200-angular.hdf5\"\n",
    "\n",
    "# h5py를 사용하여 파일 열기\n",
    "with h5py.File(file_path, 'r') as f:\n",
    "    # HDF5 파일 내의 데이터셋 키 확인 (어떤 데이터가 있는지 모를 경우 유용)\n",
    "    print(f\"Keys in HDF5 file: {list(f.keys())}\")\n",
    "\n",
    "    # 각 데이터셋을 numpy 배열로 불러오기\n",
    "    train = np.array(f['train'])\n",
    "    test = np.array(f['test'])\n",
    "    neighbors = np.array(f['neighbors'])\n",
    "    # distances 데이터셋이 있다면 같이 로드할 수 있습니다.\n",
    "    # distances = np.array(f['distances'])\n",
    "\n",
    "# random sample 100,000 from train\n",
    "K_value = 10\n",
    "seed = 42\n",
    "n_target = 10_000\n",
    "rng = np.random.RandomState(seed)\n",
    "idx = rng.choice(train.shape[0], n_target, replace=False)\n",
    "train = train[idx]\n",
    "# only use first 1,000 test vectors for quick trial\n",
    "test = test[:1000]\n",
    "neighbor_subset = exact_topk(train, test, max(100, K_value), distance_type='cosine')\n",
    "\n",
    "dim = train.shape[1]\n",
    "efConstruction = 50\n",
    "paramM = 16\n",
    "distance_method = 'cosine'\n",
    "\n",
    "# (1) Cosine ~= Inner Product 를 위해 L2 정규화\n",
    "train_norm = train.copy()\n",
    "\n",
    "# (3-A) Naive(랜덤 삽입 순서)\n",
    "rng_vis = np.random.RandomState(42)\n",
    "naive_order = rng_vis.permutation(n_target)\n",
    "\n",
    "# (3-B) Cluster-wise(클러스터 순서로 삽입)\n",
    "kmeans = KMeans(n_clusters=10, n_init='auto', random_state=21).fit(train_norm)\n",
    "cluster_data = defaultdict(list)\n",
    "for i, lbl in enumerate(kmeans.labels_):\n",
    "    cluster_data[int(lbl)].append(i)\n",
    "\n",
    "clustered_order = []\n",
    "for c in sorted(cluster_data.keys()):\n",
    "    clustered_order.extend(cluster_data[c])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in HDF5 file: ['distances', 'neighbors', 'test', 'train']\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T02:03:10.340337Z",
     "start_time": "2025-10-31T02:00:07.845411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Build custom HNSW (naive / cluster-wise) — prototype\n",
    "\n",
    "def l2_normalize_inplace(X):\n",
    "    n = np.linalg.norm(X, axis=1, keepdims=True) + 1e-12\n",
    "    X /= n\n",
    "\n",
    "# cosine 기반이면 정규화\n",
    "if distance_method == 'cosine':\n",
    "    l2_normalize_inplace(train)\n",
    "\n",
    "# 인덱스 파라미터\n",
    "custom_M = 8\n",
    "custom_ef = 128\n",
    "\n",
    "# (A) naive custom HNSW\n",
    "naive_custom = HNSW(distance_method, M=custom_M, efConstruction=custom_ef)\n",
    "for i, idx0 in enumerate(naive_order):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"[naive_custom] inserting {i}/{len(naive_order)}\")\n",
    "    naive_custom.insert(train[idx0])\n",
    "\n",
    "# (B) cluster-wise custom HNSW\n",
    "cluster_custom = HNSW(distance_method, M=custom_M, efConstruction=custom_ef)\n",
    "for i, idx0 in enumerate(clustered_order):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"[cluster_custom] inserting {i}/{len(clustered_order)}\")\n",
    "    cluster_custom.insert(train[idx0])\n",
    "\n",
    "print(\"done: custom HNSW builds\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[naive_custom] inserting 0/10000\n",
      "[naive_custom] inserting 1000/10000\n",
      "[naive_custom] inserting 2000/10000\n",
      "[naive_custom] inserting 3000/10000\n",
      "[naive_custom] inserting 4000/10000\n",
      "[naive_custom] inserting 5000/10000\n",
      "[naive_custom] inserting 6000/10000\n",
      "[naive_custom] inserting 7000/10000\n",
      "[naive_custom] inserting 8000/10000\n",
      "[naive_custom] inserting 9000/10000\n",
      "[cluster_custom] inserting 0/10000\n",
      "[cluster_custom] inserting 1000/10000\n",
      "[cluster_custom] inserting 2000/10000\n",
      "[cluster_custom] inserting 3000/10000\n",
      "[cluster_custom] inserting 4000/10000\n",
      "[cluster_custom] inserting 5000/10000\n",
      "[cluster_custom] inserting 6000/10000\n",
      "[cluster_custom] inserting 7000/10000\n",
      "[cluster_custom] inserting 8000/10000\n",
      "[cluster_custom] inserting 9000/10000\n",
      "done: custom HNSW builds\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T02:06:20.686748Z",
     "start_time": "2025-10-31T02:06:20.654933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "def ulbar_single_level(hnsw: HNSW, UL: int, q_quant=0.7, C_min=2, r_min=0.10,\n",
    "                        L_neigh=3, K_per_comm=3, Bdrop=10, Badd=10,\n",
    "                        retarget_entry=False, verbose=True) -> Tuple[int,int]:\n",
    "    \"\"\"Apply ULBAR at one level UL. Returns (#drops,#adds).\n",
    "    Uses apply_swaps_with_cap() to enforce M cap and reciprocity.\n",
    "    \"\"\"\n",
    "    lc = hnsw._graphs[UL]\n",
    "    G = nx.Graph()\n",
    "    for u, nbrs in lc.items():\n",
    "        for v in nbrs.keys():\n",
    "            if u < v:\n",
    "                G.add_edge(u, v)\n",
    "    if G.number_of_nodes() == 0:\n",
    "        return (0, 0)\n",
    "    # Retarget entry at the first (highest) level if asked\n",
    "    if retarget_entry:\n",
    "        deg_map = dict(G.degree())\n",
    "        new_ep = max(deg_map, key=deg_map.get)\n",
    "        if verbose:\n",
    "            print(f\"[ULBAR L{UL}] set entry_point -> {new_ep}\")\n",
    "        hnsw._enter_point = new_ep\n",
    "    # Communities\n",
    "    try:\n",
    "        from networkx.algorithms.community import louvain_communities\n",
    "        comms = louvain_communities(G, seed=42)\n",
    "    except Exception:\n",
    "        from networkx.algorithms.community import greedy_modularity_communities\n",
    "        comms = greedy_modularity_communities(G)\n",
    "    comm_id = {}\n",
    "    for cid, C in enumerate(comms):\n",
    "        for x in C:\n",
    "            comm_id[int(x)] = cid\n",
    "    # helpers\n",
    "    def coverage_and_ratio(u):\n",
    "        vs = list(lc.get(u, {}).keys())\n",
    "        if not vs:\n",
    "            return 0, 0.0\n",
    "        cov = len({comm_id.get(v, -1) for v in vs})\n",
    "        ic  = sum(1 for v in vs if comm_id.get(v, -1) != comm_id.get(u, -1)) / len(vs)\n",
    "        return cov, ic\n",
    "    use_l2 = (hnsw.distance_func == hnsw.l2_distance)\n",
    "    def dist_idx(a,b):\n",
    "        if use_l2:\n",
    "            return float(np.linalg.norm(hnsw.data[a] - hnsw.data[b]))\n",
    "        va, vb = hnsw.data[a], hnsw.data[b]\n",
    "        na = np.linalg.norm(va) + 1e-12\n",
    "        nb = np.linalg.norm(vb) + 1e-12\n",
    "        return 1.0 - float(np.dot(va, vb) / (na * nb))\n",
    "    def jaccard_lvl(a,b):\n",
    "        A = set(lc.get(a, {}).keys()); B = set(lc.get(b, {}).keys())\n",
    "        if not A and not B: return 0.0\n",
    "        return len(A & B) / max(len(A | B), 1)\n",
    "    # Isolation\n",
    "    iso_nodes = []\n",
    "    for u in lc.keys():\n",
    "        cov, ic = coverage_and_ratio(u)\n",
    "        if cov < C_min or ic < r_min:\n",
    "            iso_nodes.append((u, cov, ic))\n",
    "    iso_nodes.sort(key=lambda x:(x[1], x[2]))\n",
    "    # long-edge tau\n",
    "    edge_lens = []\n",
    "    seen = set()\n",
    "    for u, nbrs in lc.items():\n",
    "        for v in nbrs.keys():\n",
    "            a,b = (u,v) if u < v else (v,u)\n",
    "            if (a,b) in seen: continue\n",
    "            seen.add((a,b))\n",
    "            edge_lens.append(dist_idx(a,b))\n",
    "    edge_lens = np.array(edge_lens) if len(edge_lens) else np.array([0.0])\n",
    "    tau = float(np.quantile(edge_lens, q_quant)) if edge_lens.size>0 else 0.0\n",
    "    # bad edges (inter-comm & long & redundant)\n",
    "    bad_edges = []\n",
    "    seen.clear()\n",
    "    for u, nbrs in lc.items():\n",
    "        for v in nbrs.keys():\n",
    "            a,b = (u,v) if u < v else (v,u)\n",
    "            if (a,b) in seen: continue\n",
    "            seen.add((a,b))\n",
    "            if comm_id.get(a,-1) == comm_id.get(b,-1):\n",
    "                continue\n",
    "            d = dist_idx(a,b)\n",
    "            if d < tau:\n",
    "                continue\n",
    "            if jaccard_lvl(a,b) > 0.5:\n",
    "                bad_edges.append((a,b,d))\n",
    "    # fallback drops if none\n",
    "    if len(bad_edges) == 0:\n",
    "        intra_long = []\n",
    "        seen.clear()\n",
    "        for u, nbrs in lc.items():\n",
    "            for v in nbrs.keys():\n",
    "                a,b = (u,v) if u < v else (v,u)\n",
    "                if (a,b) in seen: continue\n",
    "                seen.add((a,b))\n",
    "                if comm_id.get(a,-1) != comm_id.get(b,-1):\n",
    "                    continue\n",
    "                intra_long.append((a,b,dist_idx(a,b)))\n",
    "        intra_long.sort(key=lambda x:-x[2])\n",
    "        bad_edges = intra_long[:Bdrop]\n",
    "    # hubs per community (degree)\n",
    "    deg = dict(G.degree())\n",
    "    per_comm = defaultdict(list)\n",
    "    for v,dv in deg.items():\n",
    "        per_comm[comm_id.get(v,-1)].append((dv,v))\n",
    "    for c in per_comm:\n",
    "        per_comm[c].sort(reverse=True)\n",
    "    # medoids\n",
    "    def medoid_of_comm(C):\n",
    "        nodes = list(C)\n",
    "        if not nodes: return None\n",
    "        sub = np.asarray([hnsw.data[i] for i in nodes])\n",
    "        if use_l2:\n",
    "            dsum = ((sub[:,None,:]-sub[None,:,:])**2).sum(axis=2).sum(axis=1)\n",
    "        else:\n",
    "            subn = sub / (np.linalg.norm(sub,axis=1,keepdims=True)+1e-12)\n",
    "            S = subn @ subn.T\n",
    "            dsum = (1.0 - S).sum(axis=1)\n",
    "        return nodes[int(np.argmin(dsum))]\n",
    "    medoids = {}\n",
    "    for cid,C in enumerate(comms):\n",
    "        m = medoid_of_comm(C)\n",
    "        if m is not None:\n",
    "            medoids[cid] = m\n",
    "    def nearest_comms_to_node(u, L=3):\n",
    "        cu = comm_id.get(u,-1)\n",
    "        pairs = []\n",
    "        for cid,m in medoids.items():\n",
    "            if cid == cu: continue\n",
    "            pairs.append((dist_idx(u,m), cid))\n",
    "        pairs.sort()\n",
    "        return [cid for _,cid in pairs[:L]]\n",
    "    # ADD proposals\n",
    "    add_props = []\n",
    "    for (u,_,_) in iso_nodes[:min(10,len(iso_nodes))]:\n",
    "        for cid in nearest_comms_to_node(u, L=L_neigh):\n",
    "            hubs = [v for _,v in per_comm.get(cid, [])[:K_per_comm]]\n",
    "            for v in hubs:\n",
    "                add_props.append((u,v,dist_idx(u,v)))\n",
    "    # build batches\n",
    "    drops_batch = [(a,b) for (a,b,_) in bad_edges[:Bdrop]]\n",
    "    adds_batch  = [(u,v) for (u,v,_) in sorted(add_props, key=lambda x:x[2])[:Badd]]\n",
    "    hnsw.apply_swaps_with_cap(UL, drops_batch, adds_batch)\n",
    "    if verbose:\n",
    "        print(f\"[ULBAR L{UL}] cap-enforced swaps: drop={len(drops_batch)}, add={len(adds_batch)}\")\n",
    "    return (len(drops_batch), len(adds_batch))\n",
    "\n",
    "\n",
    "def ulbar_multi_levels(hnsw: HNSW, mode='top_half', q_quant=0.7, budgets=(10,10), verbose=True):\n",
    "    \"\"\"Run ULBAR across multiple levels (excluding level 0).\n",
    "    mode: 'all_upper' (levels max..1), 'top_half' (from max down to max//2).\n",
    "    budgets: (Bdrop, Badd)\n",
    "    \"\"\"\n",
    "    lvl_map = hnsw.max_level_map()\n",
    "    if not lvl_map:\n",
    "        return []\n",
    "    Lmax = max(lvl_map.values())\n",
    "    if mode == 'all_upper':\n",
    "        target_lvls = list(range(Lmax, 0, -1))\n",
    "    else:  # top_half default\n",
    "        start = Lmax\n",
    "        stop  = max(1, Lmax//2)\n",
    "        target_lvls = list(range(start, stop-1, -1))\n",
    "    if verbose:\n",
    "        print(f\"[ULBAR multi] target levels: {target_lvls}\")\n",
    "    ops = []\n",
    "    for i, UL in enumerate(target_lvls):\n",
    "        drops, adds = ulbar_single_level(\n",
    "            hnsw, UL, q_quant=q_quant, Bdrop=budgets[0], Badd=budgets[1],\n",
    "            retarget_entry=(i==0), verbose=verbose)\n",
    "        ops.append((UL, drops, adds))\n",
    "    return ops"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T02:07:31.217287Z",
     "start_time": "2025-10-31T02:07:27.954093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "ULBAR (Upper-Layer Bridge Audit & Repair):\n",
    "\t•\tTOP layer 그래프 생성 → 커뮤니티 분할\n",
    "\t•\tentry point를 상층의 최대 허브로 재설정\n",
    "\t•\tisolated node 탐지: coverage<2 또는 inter-cluster<0.25\n",
    "\t•\tbad long edges: inter-comm ∧ 길이 quantile 상위(0.7) ∧ redundancy(Jaccard)>0.5\n",
    "\t•\tfallback: bad_edges가 없으면 intra-comm 장거리 상위 일부 drop\n",
    "\t•\tADD proposals: isolated 상위 10개 × 가까운 타 커뮤니티 3개 × 허브 3개\n",
    "\t•\t스왑 적용(drop/add): drop_undirected_edge / add_undirected_edge\n",
    "'''\n",
    "naive_internal_to_orig = np.array(naive_order)\n",
    "cluster_internal_to_orig = np.array(clustered_order)\n",
    "\n",
    "def map_internal_to_orig(idxs_internal, order_map):\n",
    "    return [int(order_map[i]) for i in idxs_internal]\n",
    "\n",
    "def calculate_recall(gt, pred, k=10):\n",
    "    s = set(gt[:k])\n",
    "    return sum(1 for x in pred[:k] if x in s) / float(k)\n",
    "\n",
    "def last_or_zero(seq):\n",
    "    return int(seq[-1]) if isinstance(seq, (list, tuple)) and len(seq) > 0 else 0\n",
    "\n",
    "# 기존 recall & visited 측정\n",
    "### Quick eval (custom HNSW naive_custom) — recall & visited\n",
    "efQuick = [60, 100]\n",
    "for ef in efQuick:\n",
    "    legacy_recs = []\n",
    "    legacy_visits = []\n",
    "    for i in range(min(test.shape[0], 200)):\n",
    "        q = test[i]\n",
    "        res = naive_custom.search(q, K=K_value, efSearch=ef)\n",
    "        internal_idxs = [ix for ix, _ in res]\n",
    "        orig_idxs = map_internal_to_orig(internal_idxs, naive_internal_to_orig)\n",
    "        legacy_recs.append(calculate_recall(neighbor_subset[i], orig_idxs, k=K_value))\n",
    "        legacy_visits.append(last_or_zero(naive_custom.visited_per_hop))\n",
    "    print(f\"[custom-naive before TOP repair] ef={ef}: recall@10={np.mean(legacy_recs):.4f}, visited={np.mean(legacy_visits):.1f}\")\n",
    "\n",
    "# === Run multi-layer ULBAR (top half by default) ===\n",
    "ops = ulbar_multi_levels(naive_custom, mode='top_half', q_quant=0.6, budgets=(12,12), verbose=True)\n",
    "print(\"[ULBAR multi] ops summary:\", ops)\n",
    "\n",
    "# Verify whether all nodes in upper layer meets degree cap M, that is, no node has more than M neighbors\n",
    "lvl_map = naive_custom.max_level_map()\n",
    "Lmax = max(lvl_map.values())\n",
    "start = Lmax\n",
    "stop  = max(1, Lmax//2)\n",
    "for layer in range(start,  stop-1, -1):\n",
    "    lc_after = naive_custom._graphs[layer]\n",
    "    violations = []\n",
    "    for u, nbrs in lc_after.items():\n",
    "        deg_u = len(nbrs)\n",
    "        if deg_u > naive_custom._M:\n",
    "            violations.append((u, deg_u))\n",
    "    if violations:\n",
    "        print(f\"[ULBAR] degree cap violations@L{layer}: {len(violations)} (examples) ->\", violations[:5])\n",
    "        throw_error(\"M Degree cap violations detected after ULBAR!\")\n",
    "    else:\n",
    "        print(f\"[ULBAR] all nodes meet degree cap@L{layer} (M={naive_custom._M})\")\n",
    "\n",
    "### Quick eval (custom HNSW naive_custom) — recall & visited (after TOP repair)\n",
    "efQuick = [60, 100]\n",
    "for ef in efQuick:\n",
    "    recs = []\n",
    "    visits = []\n",
    "    for i in range(min(test.shape[0], 200)):\n",
    "        q = test[i]\n",
    "        if distance_method == 'cosine':\n",
    "            q = q / (np.linalg.norm(q) + 1e-12)\n",
    "        res = naive_custom.search(q, K=K_value, efSearch=ef)\n",
    "        internal_idxs = [ix for ix, _ in res]\n",
    "        orig_idxs = map_internal_to_orig(internal_idxs, naive_internal_to_orig)\n",
    "        recs.append(calculate_recall(neighbor_subset[i], orig_idxs, k=K_value))\n",
    "        visits.append(last_or_zero(naive_custom.visited_per_hop))\n",
    "    print(f\"[custom-naive TOP repair] ef={ef}: recall@10={np.mean(recs):.4f}, visited={np.mean(visits):.1f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom-naive before TOP repair] ef=60: recall@10=0.6725, visited=503.6\n",
      "[custom-naive before TOP repair] ef=100: recall@10=0.7460, visited=768.8\n",
      "[ULBAR multi] target levels: [4, 3, 2]\n",
      "[ULBAR L3] cap-enforced swaps: drop=12, add=0\n",
      "[ULBAR L2] cap-enforced swaps: drop=12, add=12\n",
      "[ULBAR multi] ops summary: [(4, 0, 0), (3, 12, 0), (2, 12, 12)]\n",
      "[ULBAR] all nodes meet degree cap@L4 (M=8)\n",
      "[ULBAR] all nodes meet degree cap@L3 (M=8)\n",
      "[ULBAR] all nodes meet degree cap@L2 (M=8)\n",
      "[custom-naive TOP repair] ef=60: recall@10=0.6680, visited=501.8\n",
      "[custom-naive TOP repair] ef=100: recall@10=0.7455, visited=767.7\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "HUSjgR9F8F8R",
    "zv8ixRkm-vfy",
    "C6bBcvRinjxR"
   ]
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
