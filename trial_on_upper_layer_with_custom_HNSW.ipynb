{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Upper layer의 long bridge 조작 - trial1 w/ custom HNSW"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T04:00:34.707678Z",
     "start_time": "2025-11-10T04:00:33.899917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import struct\n",
    "import networkx as nx\n",
    "from heapq import heapify, heappop, heappush, heapreplace, nlargest, nsmallest\n",
    "from operator import itemgetter\n",
    "from random import random\n",
    "\n",
    "from numpy.f2py.auxfuncs import throw_error\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "\n",
    "class HNSW:\n",
    "    # self._graphs[level][i] contains a {j: dist} dictionary,\n",
    "    # where j is a neighbor of i and dist is distance\n",
    "\n",
    "    # L2 / cosine (스칼라) 함수는 기존대로 유지하되 안정성 보강\n",
    "    def l2_distance(self, a, b):\n",
    "        return float(np.linalg.norm(a - b))\n",
    "\n",
    "    def cosine_distance(self, a, b):\n",
    "        na = np.linalg.norm(a) + 1e-12\n",
    "        nb = np.linalg.norm(b) + 1e-12\n",
    "        return 1.0 - float(np.dot(a, b) / (na * nb))\n",
    "\n",
    "    def vectorized_distance_(self, x, ys):\n",
    "        ys_arr = np.asarray(ys)\n",
    "        if ys_arr.ndim == 1:\n",
    "            ys_arr = ys_arr.reshape(1, -1)\n",
    "        if self.distance_type == \"l2\":\n",
    "            # squared or actual norm: 기존 스칼라가 np.linalg.norm을 사용하므로 일관성 위해 norm 사용\n",
    "            return np.linalg.norm(ys_arr - x, axis=1)\n",
    "        elif self.distance_type == \"cosine\":\n",
    "            x_norm = x / (np.linalg.norm(x) + 1e-12)\n",
    "            ys_norm = ys_arr / (np.linalg.norm(ys_arr, axis=1, keepdims=True) + 1e-12)\n",
    "            return 1.0 - (ys_norm @ x_norm)\n",
    "        else:\n",
    "            # fallback: 호출 가능한 distance_func으로 루프\n",
    "            return np.array([self.distance_func(x, y) for y in ys_arr])\n",
    "\n",
    "    def __init__(self, distance_type, M=5, efConstruction=200, Mmax=None, heuristic=True):\n",
    "        if distance_type == \"l2\":\n",
    "            distance_func = self.l2_distance\n",
    "        elif distance_type == \"cosine\":\n",
    "            distance_func = self.cosine_distance\n",
    "        else:\n",
    "            raise TypeError('Please check your distance type!')\n",
    "        self.distance_func = distance_func\n",
    "        self.distance_type = distance_type  # 추가\n",
    "        self.vectorized_distance = self.vectorized_distance_\n",
    "        self._M = M\n",
    "        self._efConstruction = efConstruction\n",
    "        self._Mmax = 2 * M if Mmax is None else Mmax\n",
    "        self._level_mult = 1 / np.log(M)\n",
    "        self._graphs = []\n",
    "        self._enter_point = None\n",
    "        self.data = []\n",
    "        self.visited_count = 0\n",
    "\n",
    "        self._select = (\n",
    "            self._select_heuristic if heuristic else self._select_naive)\n",
    "        ##########\n",
    "        self.visited_per_hop = []\n",
    "        self.ann_per_hop = []\n",
    "        #########\n",
    "\n",
    "    ### Algorithm 1: INSERT\n",
    "    def insert(self, q, efConstruction=None):\n",
    "\n",
    "        if efConstruction is None:\n",
    "            efConstruction = self._efConstruction\n",
    "\n",
    "        distance = self.vectorized_distance_\n",
    "        data = self.data\n",
    "        graphs = self._graphs\n",
    "        ep = self._enter_point\n",
    "        M = self._M\n",
    "\n",
    "        # line 4: determine level for the new element q\n",
    "        l = int(-np.log(random()) * self._level_mult) + 1\n",
    "        idx = len(data)\n",
    "        data.append(q)\n",
    "\n",
    "        if ep is not None:\n",
    "            neg_dist = -distance(q, data[ep])\n",
    "            # distance(q, data[ep])|\n",
    "\n",
    "            # line 5-7: find the closest neighbor for levels above the insertion level\n",
    "            for lc in reversed(graphs[l:]):\n",
    "                neg_dist, ep = self._search_layer(q, [(neg_dist, ep)], lc, 1)[0]\n",
    "\n",
    "            # line 8-17: insert q at the relevant levels; W is a candidate list\n",
    "            layer0 = graphs[0]\n",
    "            W = [(neg_dist, ep)]  ## 추가\n",
    "\n",
    "            for lc in reversed(graphs[:l]):\n",
    "                M_layer = M if lc is not layer0 else self._Mmax\n",
    "\n",
    "                # line 9: update W with the closest nodes found in the graph\n",
    "                W = self._search_layer(q, W, lc, efConstruction)  ## 변경\n",
    "\n",
    "                # line 10: insert the best neighbors for q at this layer\n",
    "                lc[idx] = layer_idx = {}\n",
    "                self._select(layer_idx, W, M_layer, lc, heap=True)\n",
    "\n",
    "                # line 11-13: insert bidirectional links to the new node\n",
    "                for j, dist in layer_idx.items():\n",
    "                    self._select(lc[j], (idx, dist), M_layer, lc)\n",
    "\n",
    "        # line 18: create empty graphs for all new levels\n",
    "        for _ in range(len(graphs), l):\n",
    "            graphs.append({idx: {}})\n",
    "            self._enter_point = idx\n",
    "\n",
    "    ### Algorithm 5: K-NN-SEARCH\n",
    "    def search(self, q, K=5, efSearch=20):\n",
    "        \"\"\"Find the K points closest to q.\"\"\"\n",
    "\n",
    "        distance = self.vectorized_distance_\n",
    "        graphs = self._graphs\n",
    "        ep = self._enter_point\n",
    "        self.visited_count = 0\n",
    "\n",
    "        if ep is None:\n",
    "            raise ValueError(\"Empty graph\")\n",
    "\n",
    "        neg_dist = -distance(q, self.data[ep])\n",
    "\n",
    "        # line 1-5: search from top layers down to the second level\n",
    "        for lc in reversed(graphs[1:]):\n",
    "            neg_dist, ep = self._search_layer(q, [(neg_dist, ep)], lc, 1)[0]\n",
    "\n",
    "        ##########\n",
    "        self.visited_per_hop = []\n",
    "        self.ann_per_hop = []\n",
    "        ##########\n",
    "\n",
    "        # line 6: search with efSearch neighbors at the bottom level\n",
    "        W = self._search_layer(q, [(neg_dist, ep)], graphs[0], efSearch)\n",
    "\n",
    "        if K is not None:\n",
    "            W = nlargest(K, W)\n",
    "        else:\n",
    "            W.sort(reverse=True)\n",
    "\n",
    "        return [(idx, -md) for md, idx in W]\n",
    "\n",
    "    ### Algorithm 2: SEARCH-LAYER\n",
    "    def _search_layer(self, q, W, lc, ef):\n",
    "\n",
    "        vectorized_distance = self.vectorized_distance\n",
    "        data = self.data\n",
    "\n",
    "        # Step 1: Initialize candidate list and visited set\n",
    "        C = [(-neg_dist, idx) for neg_dist, idx in W]\n",
    "        heapify(C)\n",
    "        heapify(W)\n",
    "        visited = set(idx for _, idx in W)\n",
    "\n",
    "        # Step 4-17: Explore neighbors until candidate list is exhausted\n",
    "        while C:\n",
    "            dist, c = heappop(C)\n",
    "            furthest = -W[0][0]\n",
    "            if dist > furthest:\n",
    "                break\n",
    "            neighbors = [e for e in lc[c] if e not in visited]\n",
    "            visited.update(neighbors)\n",
    "            if neighbors:\n",
    "                # data에서 한 번에 슬라이스하여 배열 생성 후 벡터화 계산\n",
    "                ys = np.vstack([data[e] for e in neighbors])\n",
    "                dists = vectorized_distance(q, ys)\n",
    "                for e, dist in zip(neighbors, dists):\n",
    "                    self.visited_count += 1\n",
    "                    neg_dist = -float(dist)\n",
    "                    if len(W) < ef:\n",
    "                        heappush(C, (float(dist), e))\n",
    "                        heappush(W, (neg_dist, e))\n",
    "                        furthest = -W[0][0]\n",
    "                    elif dist < furthest:\n",
    "                        heappush(C, (float(dist), e))\n",
    "                        heapreplace(W, (neg_dist, e))\n",
    "                        furthest = -W[0][0]\n",
    "\n",
    "            ##########\n",
    "            self.visited_per_hop.append(len(visited))\n",
    "            topk = nsmallest(min(ef, len(W)), ((-neg, idx) for neg, idx in W))  # (dist, id)\n",
    "            self.ann_per_hop.append([idx for _, idx in topk])\n",
    "            ##########\n",
    "\n",
    "        return W\n",
    "\n",
    "    ### Algorithm 3: SELECT-NEIGHBORS-SIMPLE\n",
    "    def _select_naive(self, R, C, M, lc, heap=False):\n",
    "\n",
    "        if not heap:\n",
    "            idx, dist = C\n",
    "            if len(R) < M:\n",
    "                R[idx] = dist\n",
    "            else:\n",
    "                max_idx, max_dist = max(R.items(), key=itemgetter(1))\n",
    "                if dist < max_dist:\n",
    "                    del R[max_idx]\n",
    "                    R[idx] = dist\n",
    "            return\n",
    "\n",
    "        else:\n",
    "            C = nlargest(M, C)\n",
    "            R.update({idx: -neg_dist for neg_dist, idx in C})\n",
    "\n",
    "    def _select_heuristic(self, d, to_insert, m, g, heap=False):\n",
    "        nb_dicts = [g[idx] for idx in d]\n",
    "\n",
    "        def prioritize(idx, dist):\n",
    "            return any(nd.get(idx, float('inf')) < dist for nd in nb_dicts), dist, idx\n",
    "\n",
    "        if not heap:\n",
    "            idx, dist = to_insert\n",
    "            to_insert = [prioritize(idx, dist)]\n",
    "        else:\n",
    "            to_insert = nsmallest(m, (prioritize(idx, -mdist)\n",
    "                                      for mdist, idx in to_insert))\n",
    "\n",
    "        assert len(to_insert) > 0\n",
    "        assert not any(idx in d for _, _, idx in to_insert)\n",
    "\n",
    "        unchecked = m - len(d)\n",
    "        assert 0 <= unchecked <= m\n",
    "        to_insert, checked_ins = to_insert[:unchecked], to_insert[unchecked:]\n",
    "        to_check = len(checked_ins)\n",
    "        if to_check > 0:\n",
    "            checked_del = nlargest(to_check, (prioritize(idx, dist)\n",
    "                                              for idx, dist in d.items()))\n",
    "        else:\n",
    "            checked_del = []\n",
    "        for _, dist, idx in to_insert:\n",
    "            d[idx] = dist\n",
    "        zipped = zip(checked_ins, checked_del)\n",
    "        for (p_new, d_new, idx_new), (p_old, d_old, idx_old) in zipped:\n",
    "            if (p_old, d_old) <= (p_new, d_new):\n",
    "                break\n",
    "            del d[idx_old]\n",
    "            d[idx_new] = d_new\n",
    "            assert len(d) == m\n",
    "\n",
    "    # ======== Upper-layer graph editing helpers ========\n",
    "    def max_level_map(self):\n",
    "        \"\"\"Return dict: node -> highest level it appears in.\"\"\"\n",
    "        lvl = defaultdict(int)\n",
    "        for li, lc in enumerate(self._graphs):\n",
    "            for i in lc.keys():\n",
    "                if i not in lvl or li > lvl[i]:\n",
    "                    lvl[i] = li\n",
    "        return dict(lvl)\n",
    "\n",
    "    def node_in_level(self, level, idx):\n",
    "        return idx in self._graphs[level]\n",
    "\n",
    "    def ensure_node_in_level(self, level, idx):\n",
    "        if idx not in self._graphs[level]:\n",
    "            self._graphs[level][idx] = {}\n",
    "\n",
    "    def add_undirected_edge(self, level, u, v):\n",
    "        \"\"\"Add bidirectional edge (u,v) at given level using current distance func and M cap.\"\"\"\n",
    "        lc = self._graphs[level]\n",
    "        # ensure nodes exist\n",
    "        if u not in lc:\n",
    "            lc[u] = {}\n",
    "        if v not in lc:\n",
    "            lc[v] = {}\n",
    "        d = self.vectorized_distance_(self.data[u], self.data[v])\n",
    "        # add to u\n",
    "        self._select(lc[u], (v, d), self._M, lc, heap=False)\n",
    "        # add to v\n",
    "        self._select(lc[v], (u, d), self._M, lc, heap=False)\n",
    "\n",
    "    def drop_undirected_edge(self, level, u, v):\n",
    "        lc = self._graphs[level]\n",
    "        if u in lc and v in lc[u]:\n",
    "            del lc[u][v]\n",
    "        if v in lc and u in lc[v]:\n",
    "            del lc[v][u]\n",
    "\n",
    "    # ======== Upper-layer robust swap helpers (M 제한을 반드시 지킬수 있도록) ========\n",
    "    def _distance_idx(self, i, j):\n",
    "        return self.vectorized_distance_(self.data[i], self.data[j])\n",
    "\n",
    "    def _prune_node_to_M(self, level, u, candidate_ids):\n",
    "        \"\"\"Rebuild u's neighbor list at given level by pruning the candidate set down to M_layer using _select().\n",
    "        Returns the set of kept neighbor ids. \"\"\"\n",
    "        lc = self._graphs[level]\n",
    "        # ensure node exists\n",
    "        if u not in lc:\n",
    "            lc[u] = {}\n",
    "        # decide cap per level\n",
    "        M_layer = self._M if level != 0 else self._Mmax\n",
    "        # build C as heap items (neg_dist, idx)\n",
    "        C = []\n",
    "        for v in set(candidate_ids):\n",
    "            if v == u:\n",
    "                continue\n",
    "            d = self._distance_idx(u, v)\n",
    "            C.append((-d, v))\n",
    "        # prune\n",
    "        R = {}\n",
    "        self._select(R, C, M_layer, lc, heap=True)\n",
    "        lc[u] = dict(R)\n",
    "        return set(lc[u].keys())\n",
    "\n",
    "    def apply_swaps_with_cap(self, level, drops, adds):\n",
    "        \"\"\"Apply batched drops/adds at a level while enforcing degree cap M per node and reciprocity.\n",
    "        - drops: list of (u, v)\n",
    "        - adds : list of (u, v)\n",
    "        \"\"\"\n",
    "        lc = self._graphs[level]\n",
    "        # build per-node to_drop/to_add\n",
    "        to_drop = defaultdict(set)\n",
    "        to_add = defaultdict(set)\n",
    "        for (u, v) in drops:\n",
    "            to_drop[u].add(v)\n",
    "            to_drop[v].add(u)\n",
    "        for (u, v) in adds:\n",
    "            to_add[u].add(v)\n",
    "            to_add[v].add(u)\n",
    "        # compute pools and prune per node\n",
    "        affected = set(list(to_drop.keys()) + list(to_add.keys()))\n",
    "        # ensure nodes exist\n",
    "        for u in affected:\n",
    "            if u not in lc:\n",
    "                lc[u] = {}\n",
    "        # gather pools\n",
    "        new_neighbors = {}\n",
    "        for u in affected:\n",
    "            cur = set(lc.get(u, {}).keys())\n",
    "            pool = (cur - to_drop[u]) | to_add[u]\n",
    "            kept = self._prune_node_to_M(level, u, pool)\n",
    "            new_neighbors[u] = kept\n",
    "        # reciprocity fix: ensure symmetry\n",
    "        for u in affected:\n",
    "            for v in list(new_neighbors[u]):\n",
    "                if u not in self._graphs[level].get(v, {}):\n",
    "                    # add back with distance\n",
    "                    d = self._distance_idx(u, v)\n",
    "                    self._select(self._graphs[level][v], (u, d), self._M if level != 0 else self._Mmax,\n",
    "                                 self._graphs[level], heap=False)\n",
    "            # remove broken reciprocals\n",
    "            curv = set(self._graphs[level][u].keys())\n",
    "            for v in list(curv):\n",
    "                if u not in self._graphs[level].get(v, {}):\n",
    "                    # remove u from v if needed\n",
    "                    if v in self._graphs[level] and u in self._graphs[level][v]:\n",
    "                        del self._graphs[level][v][u]"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Preparation"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Asymmetric (BEIR: NFCorpus) — End‑to‑End embedding prep"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-05T05:47:16.734645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    return \"cpu\"\n",
    "device = get_device()\n",
    "print(f\"[asymmetric] using device: {device}\")\n",
    "\n",
    "# --- 1) Load BEIR NFCorpus (corpus & queries) ---\n",
    "# NOTE: for BEIR loaders, the split names here are 'corpus' and 'queries' (not 'train')\n",
    "print(\"[asymmetric] loading BEIR/nfcorpus (corpus, queries) ...\")\n",
    "corpus_ds  = load_dataset(\"beir/nfcorpus\", \"corpus\")\n",
    "queries_ds = load_dataset(\"beir/nfcorpus\", \"queries\")\n",
    "print(corpus_ds)   # should show key 'corpus'\n",
    "print(queries_ds)  # should show key 'queries'\n",
    "\n",
    "corpus_data  = corpus_ds[\"corpus\"]     # BEIR corpus split\n",
    "queries_data = queries_ds[\"queries\"]   # BEIR queries split\n",
    "\n",
    "# --- 2) Build texts and id maps ---\n",
    "corpus_ids = []\n",
    "corpus_texts = []\n",
    "for doc in corpus_data:\n",
    "    _id = str(doc.get(\"_id\", \"\"))\n",
    "    title = (doc.get(\"title\", \"\") or \"\").strip()\n",
    "    text  = (doc.get(\"text\", \"\")  or \"\").strip()\n",
    "    corpus_ids.append(_id)\n",
    "    corpus_texts.append((title + \" \" + text).strip())\n",
    "\n",
    "query_ids = []\n",
    "query_texts = []\n",
    "for q in queries_data:\n",
    "    _id = str(q.get(\"_id\", \"\"))\n",
    "    title = (q.get(\"title\", \"\") or \"\").strip()\n",
    "    text  = (q.get(\"text\", \"\")  or \"\").strip()\n",
    "    query_ids.append(_id)\n",
    "    # Asymmetric: queries are typically shorter; keep title+text but it's fine either way\n",
    "    query_texts.append((title + \" \" + text).strip())\n",
    "print(f\"[asymmetric] corpus docs: {len(corpus_texts)} | queries: {len(query_texts)}\")\n",
    "\n",
    "# --- 3) Load a dual‑encoder suited model (asymmetric friendly) ---\n",
    "# multi-qa-mpnet-base-dot-v1 works well for BEIR scenarios\n",
    "model_name = \"multi-qa-mpnet-base-dot-v1\"\n",
    "print(f\"[asymmetric] loading SentenceTransformer('{model_name}') ...\")\n",
    "st_model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "# --- 4) Encode corpus & queries ---\n",
    "# Tip: larger batch sizes help on GPU; reduce on CPU\n",
    "batch_corpus = 128 if device != \"cpu\" else 32\n",
    "batch_query  = 128 if device != \"cpu\" else 32\n",
    "\n",
    "t0 = time.time()\n",
    "print(\"[asymmetric] encoding corpus ...\")\n",
    "corpus_embeddings = st_model.encode(\n",
    "    corpus_texts,\n",
    "    batch_size=batch_corpus,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True  # cosine 유사도 용\n",
    ")\n",
    "t1 = time.time(); print(f\"[asymmetric] corpus done in {t1-t0:.2f}s, shape={corpus_embeddings.shape}\")\n",
    "print(\"[asymmetric] encoding queries ...\")\n",
    "query_embeddings = st_model.encode(\n",
    "    query_texts,\n",
    "    batch_size=batch_query,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True  # cosine 유사도 용\n",
    ")\n",
    "\n",
    "t2 = time.time(); print(f\"[asymmetric] queries done in {t2-t1:.2f}s, shape={query_embeddings.shape}\")\n",
    "# --- 5) Save artifacts (npy + id maps for alignment) ---\n",
    "out_dir = \"./datasets/asymmetric_nfcorpus\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "corp_npy = os.path.join(out_dir, \"nfcorpus_corpus_embeddings.npy\")\n",
    "qry_npy  = os.path.join(out_dir, \"nfcorpus_query_embeddings.npy\")\n",
    "ids_corp = os.path.join(out_dir, \"nfcorpus_corpus_ids.json\")\n",
    "ids_qry  = os.path.join(out_dir, \"nfcorpus_query_ids.json\")\n",
    "\n",
    "train = np.load(corp_npy) if os.path.exists(corp_npy) else None\n",
    "test = np.load(qry_npy) if os.path.exists(qry_npy) else None\n",
    "print(\"[asymmetric] existing corpus embeddings:\", None if train is None else train.shape)\n",
    "print(\"[asymmetric] existing query embeddings:\",  None if test is None else test.shape)\n",
    "\n",
    "np.save(corp_npy, corpus_embeddings)\n",
    "np.save(qry_npy,  query_embeddings)\n",
    "with open(ids_corp, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(corpus_ids, f, ensure_ascii=False)\n",
    "with open(ids_qry,  \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(query_ids,  f, ensure_ascii=False)\n",
    "    print(f\"[asymmetric] saved:\\n- {corp_npy}\\n- {qry_npy}\\n- {ids_corp}\\n- {ids_qry}\")\n",
    "\n",
    "# --- 6) (Optional) Quick sanity: cosine similarity of a random query to top-5 docs ---\n",
    "try:\n",
    "    import numpy as _np\n",
    "    from numpy.linalg import norm as _norm\n",
    "    ridx = np.random.randint(0, len(query_embeddings))\n",
    "    qv = query_embeddings[ridx]\n",
    "    sims = corpus_embeddings @ qv  # both normalized\n",
    "    top5 = sims.argsort()[-5:][::-1]\n",
    "    print(\"[asymmetric] sanity check — random query top5 corpus idx:\", top5.tolist())\n",
    "except Exception as e:\n",
    "    print(\"[asymmetric] sanity check skipped:\", e)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "921ae0e425914154ab6229c0137b230c"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[asymmetric] using device: mps\n",
      "[asymmetric] loading BEIR/nfcorpus (corpus, queries) ...\n",
      "DatasetDict({\n",
      "    corpus: Dataset({\n",
      "        features: ['_id', 'title', 'text'],\n",
      "        num_rows: 3633\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    queries: Dataset({\n",
      "        features: ['_id', 'title', 'text'],\n",
      "        num_rows: 3237\n",
      "    })\n",
      "})\n",
      "[asymmetric] corpus docs: 3633 | queries: 3237\n",
      "[asymmetric] loading SentenceTransformer('multi-qa-mpnet-base-dot-v1') ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ce44c5740e941f5bb869b7799869c72"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/212 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "86d52b1d265040cd83b5f34495eb1365"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5cacbed5b0e7489ab53006780b64b994"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6a2881a601c5437592d027a39552dd37"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d408f3b432054544abac27e3b1a82d25"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4eba2fa6bc9944a9b5543dbc6eb694a9"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a7218d4db1244fb9a23edefc31d5ed6"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cdc5e5a4e68643bb94c7948598e43677"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40ba804aa8524175926d45cde9bcbb56"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b3d27df56ceb49988c9c2f4a5987967b"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9cb50b80d2c54d8b9b034380611cb5b4"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[asymmetric] encoding corpus ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/29 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d05db57f417843e1b823867b59d1f762"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T04:44:59.748978Z",
     "start_time": "2025-11-05T04:44:59.744428Z"
    }
   },
   "cell_type": "markdown",
   "source": "#### Symmetric (GloVe) — End‑to‑End embedding prep + groundtruth"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T04:04:34.102984Z",
     "start_time": "2025-11-10T04:01:08.583662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Symmetric Datasets\n",
    "def read_fvecs(filename):\n",
    "    \"\"\"Reads .fvecs binary file into np.ndarray of shape (n, d).\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = f.read()\n",
    "    dim = struct.unpack('i', data[:4])[0]\n",
    "    vecs = np.frombuffer(data, dtype=np.float32)\n",
    "    vecs = vecs.reshape(-1, dim + 1)[:, 1:]  # drop the leading 'dim'\n",
    "    return vecs\n",
    "\n",
    "def read_ivecs(filename):\n",
    "    \"\"\"Reads .ivecs binary file into np.ndarray of shape (n, k).\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = f.read()\n",
    "    dim = struct.unpack('i', data[:4])[0]\n",
    "    vecs = np.frombuffer(data, dtype=np.int32)\n",
    "    vecs = vecs.reshape(-1, dim + 1)[:, 1:]\n",
    "    return vecs\n",
    "\n",
    "\n",
    "def compute_true_neighbors(data, query, k, distance_func):\n",
    "    \"\"\"Compute true k-nearest neighbors for query vectors from data.\"\"\"\n",
    "    true_neighbors = []\n",
    "    for q in query:\n",
    "        dists = np.array([distance_func(q, x) for x in data])\n",
    "        nn_indices = np.argsort(dists)[:k]\n",
    "        true_neighbors.append(nn_indices)\n",
    "    return np.array(true_neighbors)\n",
    "\n",
    "\n",
    "def exact_topk(train_subset, queries, K, distance_type='l2'):\n",
    "    out = np.empty((len(queries), K), dtype=np.int32)\n",
    "    for i, q in enumerate(queries):\n",
    "        if distance_type == 'l2':\n",
    "            d = np.sum((train_subset - q) ** 2, axis=1)\n",
    "        elif distance_type == 'angular' or distance_type == 'cosine':\n",
    "            # L2 정규화 필요\n",
    "            q_norm = q / (np.linalg.norm(q) + 1e-12)\n",
    "            train_norm = train_subset / (np.linalg.norm(train_subset, axis=1, keepdims=True) + 1e-12)\n",
    "            d = 1.0 - np.dot(train_norm, q_norm)\n",
    "        else:\n",
    "            raise ValueError(\"distance_type은 'l2' 또는 'angular'이어야 합니다.\")\n",
    "        idx = np.argpartition(d, K)[:K]\n",
    "        idx = idx[np.argsort(d[idx])]\n",
    "        out[i] = idx\n",
    "    return out\n",
    "\n",
    "\n",
    "# 데이터셋 경로 (현재 구조에 맞춰 수정)\n",
    "base_path = \"./datasets\"\n",
    "# 데이터셋 경로\n",
    "file_path = base_path + \"/nytimes-256-angular.hdf5\"\n",
    "\n",
    "# train = read_fvecs(f\"{base_path}/sift_base.fvecs\")  # 1,000,000 × 128\n",
    "# print(f\"Loaded train dataset with shape: {train.shape}\")\n",
    "# test = read_fvecs(f\"{base_path}/sift_query.fvecs\")  # 10,000 × 128\n",
    "# print(f\"Loaded test dataset with shape: {test.shape}\")\n",
    "# neighbors = read_ivecs(f\"{base_path}/sift_groundtruth.ivecs\")  # 10,000 × 100\n",
    "# print(f\"Loaded neighbors dataset with shape: {neighbors.shape}\")\n",
    "\n",
    "\n",
    "# h5py를 사용하여 파일 열기\n",
    "with h5py.File(file_path, 'r') as f:\n",
    "    # HDF5 파일 내의 데이터셋 키 확인 (어떤 데이터가 있는지 모를 경우 유용)\n",
    "    print(f\"Keys in HDF5 file: {list(f.keys())}\")\n",
    "\n",
    "    # 각 데이터셋을 numpy 배열로 불러오기\n",
    "    train = np.array(f['train'])\n",
    "    test = np.array(f['test'])\n",
    "    neighbors = np.array(f['neighbors'])\n",
    "    # distances 데이터셋이 있다면 같이 로드할 수 있습니다.\n",
    "    # distances = np.array(f['distances'])\n",
    "\n",
    "# random sample 100,000 from train\n",
    "K_value = 10\n",
    "seed = 42\n",
    "n_target = 100_000\n",
    "rng = np.random.RandomState(seed)\n",
    "idx = rng.choice(train.shape[0], n_target, replace=False)\n",
    "train = train[idx]\n",
    "test = test[:3000]\n",
    "neighbor_subset = exact_topk(train, test, max(100, K_value), distance_type='cosine')\n",
    "\n",
    "dim = train.shape[1]\n",
    "efConstruction = 100\n",
    "paramM = 16\n",
    "distance_method = 'cosine'\n",
    "\n",
    "# (1) Cosine ~= Inner Product 를 위해 L2 정규화\n",
    "train_norm = train.copy()\n",
    "\n",
    "# (3-A) Naive(랜덤 삽입 순서)\n",
    "rng_vis = np.random.RandomState(42)\n",
    "naive_order = rng_vis.permutation(n_target)\n",
    "\n",
    "# # (3-B) Cluster-wise(클러스터 순서로 삽입)\n",
    "# kmeans = KMeans(n_clusters=10, n_init='auto', random_state=21).fit(train_norm)\n",
    "# cluster_data = defaultdict(list)\n",
    "# for i, lbl in enumerate(kmeans.labels_):\n",
    "#     cluster_data[int(lbl)].append(i)\n",
    "#\n",
    "# clustered_order = []\n",
    "# for c in sorted(cluster_data.keys()):\n",
    "#     clustered_order.extend(cluster_data[c])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in HDF5 file: ['distances', 'neighbors', 'test', 'train']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T04:32:17.448132Z",
     "start_time": "2025-11-10T04:05:36.244033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Build custom HNSW (naive / cluster-wise) — prototype\n",
    "\n",
    "def l2_normalize_inplace(X):\n",
    "    n = np.linalg.norm(X, axis=1, keepdims=True) + 1e-12\n",
    "    X /= n\n",
    "\n",
    "\n",
    "# cosine 기반이면 정규화\n",
    "if distance_method == 'cosine':\n",
    "    l2_normalize_inplace(train)\n",
    "\n",
    "# (A) naive custom HNSW\n",
    "naive_custom = HNSW(distance_method, M=paramM, efConstruction=efConstruction)\n",
    "for i, idx0 in enumerate(naive_order):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"[naive_custom] inserting {i}/{len(naive_order)}\")\n",
    "    naive_custom.insert(train[idx0])\n",
    "#\n",
    "# # (B) cluster-wise custom HNSW\n",
    "# cluster_custom = HNSW(distance_method, M=paramM, efConstruction=efConstruction)\n",
    "# for i, idx0 in enumerate(clustered_order):\n",
    "#     if i % 1000 == 0:\n",
    "#         print(f\"[cluster_custom] inserting {i}/{len(clustered_order)}\")\n",
    "#     cluster_custom.insert(train[idx0])\n",
    "\n",
    "print(\"done: custom HNSW builds\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[naive_custom] inserting 0/100000\n",
      "[naive_custom] inserting 1000/100000\n",
      "[naive_custom] inserting 2000/100000\n",
      "[naive_custom] inserting 3000/100000\n",
      "[naive_custom] inserting 4000/100000\n",
      "[naive_custom] inserting 5000/100000\n",
      "[naive_custom] inserting 6000/100000\n",
      "[naive_custom] inserting 7000/100000\n",
      "[naive_custom] inserting 8000/100000\n",
      "[naive_custom] inserting 9000/100000\n",
      "[naive_custom] inserting 10000/100000\n",
      "[naive_custom] inserting 11000/100000\n",
      "[naive_custom] inserting 12000/100000\n",
      "[naive_custom] inserting 13000/100000\n",
      "[naive_custom] inserting 14000/100000\n",
      "[naive_custom] inserting 15000/100000\n",
      "[naive_custom] inserting 16000/100000\n",
      "[naive_custom] inserting 17000/100000\n",
      "[naive_custom] inserting 18000/100000\n",
      "[naive_custom] inserting 19000/100000\n",
      "[naive_custom] inserting 20000/100000\n",
      "[naive_custom] inserting 21000/100000\n",
      "[naive_custom] inserting 22000/100000\n",
      "[naive_custom] inserting 23000/100000\n",
      "[naive_custom] inserting 24000/100000\n",
      "[naive_custom] inserting 25000/100000\n",
      "[naive_custom] inserting 26000/100000\n",
      "[naive_custom] inserting 27000/100000\n",
      "[naive_custom] inserting 28000/100000\n",
      "[naive_custom] inserting 29000/100000\n",
      "[naive_custom] inserting 30000/100000\n",
      "[naive_custom] inserting 31000/100000\n",
      "[naive_custom] inserting 32000/100000\n",
      "[naive_custom] inserting 33000/100000\n",
      "[naive_custom] inserting 34000/100000\n",
      "[naive_custom] inserting 35000/100000\n",
      "[naive_custom] inserting 36000/100000\n",
      "[naive_custom] inserting 37000/100000\n",
      "[naive_custom] inserting 38000/100000\n",
      "[naive_custom] inserting 39000/100000\n",
      "[naive_custom] inserting 40000/100000\n",
      "[naive_custom] inserting 41000/100000\n",
      "[naive_custom] inserting 42000/100000\n",
      "[naive_custom] inserting 43000/100000\n",
      "[naive_custom] inserting 44000/100000\n",
      "[naive_custom] inserting 45000/100000\n",
      "[naive_custom] inserting 46000/100000\n",
      "[naive_custom] inserting 47000/100000\n",
      "[naive_custom] inserting 48000/100000\n",
      "[naive_custom] inserting 49000/100000\n",
      "[naive_custom] inserting 50000/100000\n",
      "[naive_custom] inserting 51000/100000\n",
      "[naive_custom] inserting 52000/100000\n",
      "[naive_custom] inserting 53000/100000\n",
      "[naive_custom] inserting 54000/100000\n",
      "[naive_custom] inserting 55000/100000\n",
      "[naive_custom] inserting 56000/100000\n",
      "[naive_custom] inserting 57000/100000\n",
      "[naive_custom] inserting 58000/100000\n",
      "[naive_custom] inserting 59000/100000\n",
      "[naive_custom] inserting 60000/100000\n",
      "[naive_custom] inserting 61000/100000\n",
      "[naive_custom] inserting 62000/100000\n",
      "[naive_custom] inserting 63000/100000\n",
      "[naive_custom] inserting 64000/100000\n",
      "[naive_custom] inserting 65000/100000\n",
      "[naive_custom] inserting 66000/100000\n",
      "[naive_custom] inserting 67000/100000\n",
      "[naive_custom] inserting 68000/100000\n",
      "[naive_custom] inserting 69000/100000\n",
      "[naive_custom] inserting 70000/100000\n",
      "[naive_custom] inserting 71000/100000\n",
      "[naive_custom] inserting 72000/100000\n",
      "[naive_custom] inserting 73000/100000\n",
      "[naive_custom] inserting 74000/100000\n",
      "[naive_custom] inserting 75000/100000\n",
      "[naive_custom] inserting 76000/100000\n",
      "[naive_custom] inserting 77000/100000\n",
      "[naive_custom] inserting 78000/100000\n",
      "[naive_custom] inserting 79000/100000\n",
      "[naive_custom] inserting 80000/100000\n",
      "[naive_custom] inserting 81000/100000\n",
      "[naive_custom] inserting 82000/100000\n",
      "[naive_custom] inserting 83000/100000\n",
      "[naive_custom] inserting 84000/100000\n",
      "[naive_custom] inserting 85000/100000\n",
      "[naive_custom] inserting 86000/100000\n",
      "[naive_custom] inserting 87000/100000\n",
      "[naive_custom] inserting 88000/100000\n",
      "[naive_custom] inserting 89000/100000\n",
      "[naive_custom] inserting 90000/100000\n",
      "[naive_custom] inserting 91000/100000\n",
      "[naive_custom] inserting 92000/100000\n",
      "[naive_custom] inserting 93000/100000\n",
      "[naive_custom] inserting 94000/100000\n",
      "[naive_custom] inserting 95000/100000\n",
      "[naive_custom] inserting 96000/100000\n",
      "[naive_custom] inserting 97000/100000\n",
      "[naive_custom] inserting 98000/100000\n",
      "[naive_custom] inserting 99000/100000\n",
      "done: custom HNSW builds\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T04:32:17.617360Z",
     "start_time": "2025-11-10T04:32:17.573273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "def ulbar_single_level(hnsw: HNSW, UL: int, q_quant=0.7, C_min=2, r_min=0.10,\n",
    "                       L_neigh=3, K_per_comm=3, Bdrop=10, Badd=10, jaccard_thresh=0.5,\n",
    "                       retarget_entry=False, verbose=True,\n",
    "                       lid_k: int = 15, lid_prop: float = 0.10, indeg_quant: float = 0.30,\n",
    "                       layer0_add_only: bool = False) -> Tuple[int, int]:\n",
    "    \"\"\"Apply ULBAR at one level UL. Returns (#drops,#adds).\n",
    "    Uses apply_swaps_with_cap() to enforce M cap and reciprocity.\n",
    "    \"\"\"\n",
    "    lc = hnsw._graphs[UL]\n",
    "    G  = nx.Graph()\n",
    "    node_set = set(lc.keys())\n",
    "    for u, nbrs in lc.items():\n",
    "        node_set.add(u)\n",
    "        for v in nbrs.keys():\n",
    "            node_set.add(v)\n",
    "    G.add_nodes_from(node_set)\n",
    "    for u, nbrs in lc.items():\n",
    "        for v in nbrs.keys():\n",
    "            if u < v:\n",
    "                dist = hnsw.vectorized_distance_(hnsw.data[u], hnsw.data[v])\n",
    "                G.add_edge(u, v, weight=1.0/(dist+1e-12))\n",
    "\n",
    "    if G.number_of_nodes() == 0:\n",
    "        print(\"no nodes found for level\", UL)\n",
    "        return (0, 0)\n",
    "    # Compute degree map once for indegree filtering\n",
    "    deg_map = {u: len(nbrs) for u, nbrs in lc.items()}\n",
    "    # # Retarget entry at the first (highest) level if asked\n",
    "    # if retarget_entry:\n",
    "    #     deg_map = dict(G.degree())\n",
    "    #     new_ep = max(deg_map, key=deg_map.get)\n",
    "    #     if verbose:\n",
    "    #         print(f\"[ULBAR L{UL}] set entry_point -> {new_ep}\")\n",
    "    #     hnsw._enter_point = new_ep\n",
    "    # Communities (KMeans on node embeddings)\n",
    "    # Heuristic for number of clusters: k = clamp(2, sqrt(|V|), 64)\n",
    "    nodes_list = list(G.nodes())\n",
    "    n_nodes = len(nodes_list)\n",
    "    k_guess = int(np.sqrt(max(2, n_nodes)))\n",
    "    k = max(1, min(128, k_guess))\n",
    "    X = np.asarray([hnsw.data[i] for i in nodes_list], dtype=float)\n",
    "    # Ensure normalization consistent with distance metric (cosine expects L2-normalized)\n",
    "    use_l2 = (hnsw.distance_func == hnsw.l2_distance)\n",
    "    if not use_l2:\n",
    "        X = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-12)\n",
    "    km = KMeans(n_clusters=k, n_init='auto' if hasattr(KMeans, \"__doc__\") else 10, random_state=42)\n",
    "    lbl = km.fit_predict(X)\n",
    "    # Build communities as sets of node ids\n",
    "    comms = []\n",
    "    for cid in range(k):\n",
    "        members = [nodes_list[i] for i in range(n_nodes) if int(lbl[i]) == cid]\n",
    "        if members:\n",
    "            comms.append(set(int(x) for x in members))\n",
    "    comm_id = {}\n",
    "    for cid, C in enumerate(comms):\n",
    "        for x in C:\n",
    "            comm_id[int(x)] = cid\n",
    "    print(f\"[ULBAR L{UL}] detected {len(comms)} communities (KMeans, k={k})\")\n",
    "    # helpers\n",
    "    def coverage_and_ratio(u):\n",
    "        vs = list(lc.get(u, {}).keys())\n",
    "        if not vs:\n",
    "            return 0, 0.0\n",
    "        # u의 이웃들이 몇 개의 커뮤니티에 속하는지\n",
    "        cov = len({comm_id.get(v, -1) for v in vs})\n",
    "        # v가 속한 커뮤니티와 u의 커뮤니티가 다른 이웃 비율 (얼마나 겹치는가?)\n",
    "        ic = sum(1 for v in vs if comm_id.get(v, -1) != comm_id.get(u, -1)) / len(vs)\n",
    "        return cov, ic\n",
    "\n",
    "    use_l2 = (hnsw.distance_func == hnsw.l2_distance)\n",
    "\n",
    "    def dist_idx(a, b):\n",
    "        if use_l2:\n",
    "            return float(np.linalg.norm(hnsw.data[a] - hnsw.data[b]))\n",
    "        va, vb = hnsw.data[a], hnsw.data[b]\n",
    "        na = np.linalg.norm(va) + 1e-12\n",
    "        nb = np.linalg.norm(vb) + 1e-12\n",
    "        return 1.0 - float(np.dot(va, vb) / (na * nb))\n",
    "\n",
    "    def jaccard_lvl(a, b):\n",
    "        A = set(lc.get(a, {}).keys());\n",
    "        B = set(lc.get(b, {}).keys())\n",
    "        if not A and not B: return 0.0\n",
    "        return len(A & B) / max(len(A | B), 1)\n",
    "\n",
    "    # Isolation\n",
    "    # iso_nodes = []\n",
    "    # for u in lc.keys():\n",
    "    #     cov, ic = coverage_and_ratio(u)\n",
    "    #     # C_min보다 더 적은 커뮤니티에 속해있는 노드 또는 inter-cluster 비율이 r_min보다 작은 노드는 isolated로 간주\n",
    "    #     if cov < C_min or ic < r_min:\n",
    "    #         iso_nodes.append((u, cov, ic))\n",
    "    # print(f\"[ULBAR L{UL}] isolated nodes detected: {len(iso_nodes)}\")\n",
    "    # iso_nodes.sort(key=lambda x: (x[1], x[2]))\n",
    "    # long-edge tau\n",
    "    edge_lens = []\n",
    "    seen = set()\n",
    "    for u, nbrs in lc.items():\n",
    "        for v in nbrs.keys():\n",
    "            a, b = (u, v) if u < v else (v, u)\n",
    "            if (a, b) in seen: continue\n",
    "            seen.add((a, b))\n",
    "            edge_lens.append(dist_idx(a, b))\n",
    "    edge_lens = np.array(edge_lens) if len(edge_lens) else np.array([0.0])\n",
    "    # 전체 edge 길이 분포 중 q_quant 분위수 값을 tau로 설정 (0.7이면 상위 30% 길이)\n",
    "    tau = float(np.quantile(edge_lens, q_quant)) if edge_lens.size > 0 else 0.0\n",
    "    # bad edges (inter-comm & long & redundant)\n",
    "    bad_edges = []\n",
    "    seen.clear()\n",
    "    for u, nbrs in lc.items():\n",
    "        for v in nbrs.keys():\n",
    "            a, b = (u, v) if u < v else (v, u)\n",
    "            if (a, b) in seen: continue\n",
    "            seen.add((a, b))\n",
    "            if comm_id.get(a, -1) == comm_id.get(b, -1):\n",
    "                continue\n",
    "            d = dist_idx(a, b)\n",
    "            # long edge들만 bad edge 후보로 간주\n",
    "            if d < tau:\n",
    "                continue\n",
    "            # 이웃 노드가 유사한 경우, 즉 같은 이웃을 많이 공유하는 경우 bad edge로 간주\n",
    "            if jaccard_lvl(a, b) > jaccard_thresh:\n",
    "                bad_edges.append((a, b, d))\n",
    "    # fallback drops if none\n",
    "    if len(bad_edges) == 0:\n",
    "        intra_long = []\n",
    "        seen.clear()\n",
    "        for u, nbrs in lc.items():\n",
    "            for v in nbrs.keys():\n",
    "                a, b = (u, v) if u < v else (v, u)\n",
    "                if (a, b) in seen: continue\n",
    "                seen.add((a, b))\n",
    "                if comm_id.get(a, -1) != comm_id.get(b, -1):\n",
    "                    continue\n",
    "                intra_long.append((a, b, dist_idx(a, b)))\n",
    "        intra_long.sort(key=lambda x: -x[2])\n",
    "        bad_edges = intra_long[:Bdrop]\n",
    "    # For layer 0 policy, bypass drops and only add edges to nearest communities\n",
    "    if layer0_add_only or UL == 0:\n",
    "        bad_edges = []  # disable drops on L0 by policy\n",
    "    # hubs per community (degree)\n",
    "    deg = dict(G.degree())\n",
    "    per_comm = defaultdict(list)\n",
    "    for v, dv in deg.items():\n",
    "        per_comm[comm_id.get(v, -1)].append((dv, v))\n",
    "    for c in per_comm:\n",
    "        per_comm[c].sort(reverse=True)\n",
    "\n",
    "    # medoids\n",
    "    def medoid_of_comm(C):\n",
    "        nodes = list(C)\n",
    "        if not nodes: return None\n",
    "        sub = np.asarray([hnsw.data[i] for i in nodes])\n",
    "        if use_l2:\n",
    "            dsum = ((sub[:, None, :] - sub[None, :, :]) ** 2).sum(axis=2).sum(axis=1)\n",
    "        else:\n",
    "            subn = sub / (np.linalg.norm(sub, axis=1, keepdims=True) + 1e-12)\n",
    "            S = subn @ subn.T\n",
    "            dsum = (1.0 - S).sum(axis=1)\n",
    "        return nodes[int(np.argmin(dsum))]\n",
    "\n",
    "    medoids = {}\n",
    "    for cid, C in enumerate(comms):\n",
    "        m = medoid_of_comm(C)\n",
    "        if m is not None:\n",
    "            medoids[cid] = m\n",
    "\n",
    "    def nearest_comms_to_node(u, L=3):\n",
    "        cu = comm_id.get(u, -1)\n",
    "        pairs = []\n",
    "        for cid, m in medoids.items():\n",
    "            if cid == cu: continue\n",
    "            pairs.append((dist_idx(u, m), cid))\n",
    "        pairs.sort()\n",
    "        # print(f\"[ULBAR L{UL}] node {u} nearest_comms (dist, cid): {pairs[:L]}\")\n",
    "        return [cid for _, cid in pairs[:L]]\n",
    "\n",
    "    def furthest_comms_to_node(u, L=3):\n",
    "        cu = comm_id.get(u, -1)\n",
    "        pairs = []\n",
    "        for cid, m in medoids.items():\n",
    "            if cid == cu: continue\n",
    "            pairs.append((dist_idx(u, m), cid))\n",
    "        pairs.sort(reverse=True)\n",
    "        return [cid for _, cid in pairs[:L]]\n",
    "\n",
    "    # ---- LID estimation on current level graph (k-NN over existing neighbors)\n",
    "    def estimate_lid_knn_level(lc_dict, k=15):\n",
    "        lid = {}\n",
    "        for u, nbrs in lc_dict.items():\n",
    "            neigh = list(nbrs.keys())\n",
    "            if not neigh:\n",
    "                lid[u] = 0.0\n",
    "                continue\n",
    "            kk = min(k, len(neigh))\n",
    "            # distances from u to its neighbors\n",
    "            ds = np.array([dist_idx(u, v) for v in neigh], dtype=float)\n",
    "            # take k smallest\n",
    "            part = np.argpartition(ds, kk-1)[:kk]\n",
    "            dk = np.sort(ds[part])\n",
    "            r_k = float(dk[-1]) + 1e-12\n",
    "            s = float(np.log(r_k / (dk + 1e-12)).mean())\n",
    "            lid[u] = (1.0 / max(s, 1e-12)) if s > 0 else 0.0\n",
    "        return lid\n",
    "\n",
    "    lid_vals = estimate_lid_knn_level(lc, k=lid_k)\n",
    "    n_layer = max(1, len(lc))\n",
    "    # # small indegree threshold (quantile)\n",
    "    # indeg_arr = np.array([deg_map.get(u, 0) for u in lc.keys()], dtype=float)\n",
    "    # indeg_thr = float(np.quantile(indeg_arr, indeg_quant)) if indeg_arr.size > 0 else 0.0\n",
    "    # small_indeg = {u for u in lc.keys() if deg_map.get(u, 0) <= indeg_thr}\n",
    "\n",
    "    # pick among small indegree nodes: top lid_prop by LID\n",
    "    candidates = [(u, lid_vals.get(u, 0.0)) for u in lid_vals]\n",
    "    candidates.sort(key=lambda t: t[1], reverse=True)\n",
    "    n_pick = max(1, int(lid_prop * n_layer))\n",
    "    top_by_lid_smalldeg = candidates[:n_pick]\n",
    "\n",
    "    iso_nodes = []\n",
    "    for u, lid_u in top_by_lid_smalldeg:\n",
    "        cov, ic = coverage_and_ratio(u)  # keep for logging/analysis\n",
    "        iso_nodes.append((u, cov, ic, lid_u, deg_map.get(u, 0)))\n",
    "    if verbose:\n",
    "        print(f\"[ULBAR L{UL}] LID-top∩small-indegree candidates: {len(iso_nodes)} (prop={lid_prop:.3f}, k={lid_k}\")\n",
    "    add_props = []\n",
    "\n",
    "    # for layer zero, only add edges to nearest communities\n",
    "    if layer0_add_only or UL == 0:\n",
    "        for (u, _, _, _, _) in iso_nodes:\n",
    "            for cid in nearest_comms_to_node(u, L=L_neigh):\n",
    "                hubs = [v for _, v in per_comm.get(cid, [])[:K_per_comm]]\n",
    "                for v in hubs:\n",
    "                    add_props.append((u, v, dist_idx(u, v)))\n",
    "    # otherwise, add edges to furthest communities\n",
    "    else:\n",
    "        for (u, _, _, _, _) in iso_nodes:\n",
    "            for cid in furthest_comms_to_node(u, L=L_neigh):\n",
    "                hubs = [v for _, v in per_comm.get(cid, [])[:K_per_comm]]\n",
    "                for v in hubs:\n",
    "                    add_props.append((v, u, dist_idx(u, v)))\n",
    "\n",
    "    # # ADD proposals\n",
    "    # add_props = []\n",
    "    # # for (u,_,_) in iso_nodes[:min(30,len(iso_nodes))]:\n",
    "    # for (u, _, _) in iso_nodes[:len(iso_nodes)]:\n",
    "    #     for cid in nearest_comms_to_node(u, L=L_neigh):\n",
    "    #         hubs = [v for _, v in per_comm.get(cid, [])[:K_per_comm]]\n",
    "    #         for v in hubs:\n",
    "    #             add_props.append((u, v, dist_idx(u, v)))\n",
    "    # build batches\n",
    "    nL = max(1, len(lc))\n",
    "    Badd_eff = max(Badd, int(0.005 * nL))\n",
    "    Bdrop_eff = 0 if (layer0_add_only or UL == 0) else max(Bdrop, int(0.005 * nL))\n",
    "    drops_batch = [(a, b) for (a, b, _) in sorted(bad_edges, key=lambda x: -x[2])[:Bdrop_eff]]\n",
    "    adds_batch = [(u, v) for (u, v, _) in sorted(add_props, key=lambda x: x[2])[:Badd_eff]]\n",
    "    hnsw.apply_swaps_with_cap(UL, drops_batch, adds_batch)\n",
    "    if verbose:\n",
    "        print(f\"[ULBAR L{UL}] cap-enforced swaps: drop={len(drops_batch)}, add={len(adds_batch)}\")\n",
    "    return (len(drops_batch), len(adds_batch))\n",
    "\n",
    "\n",
    "def ulbar_multi_levels(hnsw: HNSW, mode='top_half', q_quant=0.7, budgets=(10, 10), jaccard_thresh=0.5, C_min=2,\n",
    "                       r_min=0.1, verbose=True, lid_k: int = 15, lid_prop: float = 0.10, indeg_quant: float = 0.30, layer0_add_only: bool = True):\n",
    "    \"\"\"Run ULBAR across multiple levels (excluding level 0).\n",
    "    mode: 'all_upper' (levels max..1), 'top_half' (from max down to max//2).\n",
    "    budgets: (Bdrop, Badd)\n",
    "    \"\"\"\n",
    "    lvl_map = hnsw.max_level_map()\n",
    "    if not lvl_map:\n",
    "        return []\n",
    "    Lmax = max(lvl_map.values())\n",
    "    if mode == 'only_L0':\n",
    "        target_lvls = [0]\n",
    "    elif mode == 'all_upper':\n",
    "        target_lvls = list(range(Lmax, 0, -1))\n",
    "    elif mode  == 'upper_half':  # top_half default\n",
    "        start = Lmax\n",
    "        stop = max(1, Lmax // 2)\n",
    "        target_lvls = list(range(start, stop - 1, -1))\n",
    "    else : # all layers\n",
    "        target_lvls = list(range(Lmax, -1, -1))\n",
    "    if verbose:\n",
    "        print(f\"[ULBAR multi] target levels: {target_lvls}\")\n",
    "    ops = []\n",
    "    for i, UL in enumerate(target_lvls):\n",
    "        drops, adds = ulbar_single_level(\n",
    "            hnsw, UL, q_quant=q_quant, Bdrop=budgets[0], Badd=budgets[1], C_min=C_min, r_min=r_min,\n",
    "            retarget_entry=(i == 0), jaccard_thresh=jaccard_thresh, verbose=verbose,\n",
    "            lid_k=lid_k, lid_prop=lid_prop, indeg_quant=indeg_quant, layer0_add_only=layer0_add_only)\n",
    "        ops.append((UL, drops, adds))\n",
    "    return ops\n",
    "\n",
    "\n",
    "# === L1-anchored L0 bridge (reverse direction: for each L1 node, attach L0 nodes that are close, LID-high, low indegree) ===\n",
    "def l1_anchor_bridge(hnsw: HNSW,\n",
    "                     lid_k: int = 15,\n",
    "                     indeg_quant: float = 0.30,\n",
    "                     candidate_prop: float = 0.30,\n",
    "                     per_l1_limit: int = 3,\n",
    "                     verbose: bool = True) -> int:\n",
    "    \"\"\"\n",
    "    For each L1 node v, add edges at L0 from v to multiple L0 nodes u that are:\n",
    "      - near v (distance small at L0),\n",
    "      - LID-high (sparse in local sense),\n",
    "      - indegree-low (few neighbors on L0).\n",
    "    Only ADDs on L0 are performed (no drops).\n",
    "    Returns the number of edges added.\n",
    "    \"\"\"\n",
    "    # Sanity: need L1 and L0 graphs\n",
    "    if len(hnsw._graphs) < 2:\n",
    "        if verbose: print(\"[L1-anchored] no level-1 graph; nothing to do.\")\n",
    "        return 0\n",
    "    lc0 = hnsw._graphs[0]\n",
    "    lc1 = hnsw._graphs[1]\n",
    "    L1_nodes = set(lc1.keys())\n",
    "    if not L1_nodes or not lc0:\n",
    "        if verbose: print(\"[L1-anchored] empty level(s); nothing to do.\")\n",
    "        return 0\n",
    "\n",
    "    # Distance helper consistent with index\n",
    "    use_l2 = (hnsw.distance_func == hnsw.l2_distance)\n",
    "    def dist_idx(a, b):\n",
    "        if use_l2:\n",
    "            return float(np.linalg.norm(hnsw.data[a] - hnsw.data[b]))\n",
    "        va, vb = hnsw.data[a], hnsw.data[b]\n",
    "        na = np.linalg.norm(va) + 1e-12\n",
    "        nb = np.linalg.norm(vb) + 1e-12\n",
    "        return 1.0 - float(np.dot(va, vb) / (na * nb))\n",
    "\n",
    "    # L0 indegree map and threshold\n",
    "    deg_map = {u: len(nbrs) for u, nbrs in lc0.items()}\n",
    "    indeg_arr = np.array([deg_map.get(u, 0) for u in lc0.keys()], dtype=float)\n",
    "    indeg_thr = float(np.quantile(indeg_arr, indeg_quant)) if indeg_arr.size > 0 else 0.0\n",
    "    small_indeg = [u for u in lc0.keys() if deg_map.get(u, 0) <= indeg_thr]\n",
    "\n",
    "    # LID estimation over L0\n",
    "    def estimate_lid_knn_level(lc_dict, k=15):\n",
    "        lid = {}\n",
    "        for u, nbrs in lc_dict.items():\n",
    "            neigh = list(nbrs.keys())\n",
    "            if not neigh:\n",
    "                lid[u] = 0.0\n",
    "                continue\n",
    "            kk = min(k, len(neigh))\n",
    "            ds = np.array([dist_idx(u, v) for v in neigh], dtype=float)\n",
    "            part = np.argpartition(ds, kk-1)[:kk]\n",
    "            dk = np.sort(ds[part])\n",
    "            r_k = float(dk[-1]) + 1e-12\n",
    "            s = float(np.log(r_k / (dk + 1e-12)).mean())\n",
    "            lid[u] = (1.0 / max(s, 1e-12)) if s > 0 else 0.0\n",
    "        return lid\n",
    "    lid_vals = estimate_lid_knn_level(lc0, k=lid_k)\n",
    "\n",
    "    # Candidate pool on L0: small indegree, then top by LID (limit by candidate_prop for speed)\n",
    "    n0 = max(1, len(lc0))\n",
    "    pool = [(u, lid_vals.get(u, 0.0), deg_map.get(u, 0)) for u in small_indeg]\n",
    "    pool.sort(key=lambda t: (-t[1], t[2]))  # LID desc, indegree asc\n",
    "    max_pool = max(1, int(candidate_prop * n0))\n",
    "    pool = pool[:max_pool]\n",
    "    pool_nodes = [u for (u, _, _) in pool]\n",
    "\n",
    "    # Build ADDs per L1 node\n",
    "    adds = []\n",
    "    for v in L1_nodes:\n",
    "        # ensure L0 presence for v\n",
    "        hnsw.ensure_node_in_level(0, v)\n",
    "        forbid = set(lc0.get(v, {}).keys()) | {v}\n",
    "        # distance to candidates not already connected\n",
    "        cand = [u for u in pool_nodes if u not in forbid]\n",
    "        if not cand:\n",
    "            continue\n",
    "        # Sort by distance asc, then -LID, then indegree\n",
    "        scored = []\n",
    "        for u in cand:\n",
    "            d = dist_idx(u, v)\n",
    "            scored.append((d, -lid_vals.get(u, 0.0), deg_map.get(u, 0), u))\n",
    "        scored.sort(key=lambda t: (t[0], t[1], t[2]))\n",
    "        # take top per_l1_limit\n",
    "        take = min(per_l1_limit, len(scored))\n",
    "        for i in range(take):\n",
    "            _, _, _, u = scored[i]\n",
    "            adds.append((v, u))  # add edge (v,u) at L0\n",
    "\n",
    "    if not adds:\n",
    "        if verbose: print(\"[L1-anchored] no adds generated.\")\n",
    "        return 0\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[L1-anchored] L1_nodes={len(L1_nodes)}, pool_L0={len(pool_nodes)}, adds={len(adds)}, indeg_thr={indeg_thr:.1f}\")\n",
    "    # Apply only at L0 (drops=[])\n",
    "    hnsw.apply_swaps_with_cap(0, [], adds)\n",
    "    return len(adds)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T04:32:17.653389Z",
     "start_time": "2025-11-10T04:32:17.648216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def map_internal_to_orig(idxs_internal, order_map):\n",
    "    return [int(order_map[i]) for i in idxs_internal]\n",
    "\n",
    "\n",
    "def calculate_recall(gt, pred, k=10):\n",
    "    s = set(gt[:k])\n",
    "    return sum(1 for x in pred[:k] if x in s) / float(k)\n",
    "\n",
    "\n",
    "def last_or_zero(seq):\n",
    "    return int(seq[-1]) if isinstance(seq, (list, tuple)) and len(seq) > 0 else 0\n",
    "\n",
    "\n",
    "naive_internal_to_orig = np.array(naive_order)\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "def clone_hnsw(hnsw_obj: HNSW) -> HNSW:\n",
    "    \"\"\"Deep-clone HNSW via pickle so we can rollback-free test variants.\"\"\"\n",
    "    return pickle.loads(pickle.dumps(hnsw_obj, protocol=pickle.HIGHEST_PROTOCOL))\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T05:10:34.933857Z",
     "start_time": "2025-11-10T04:53:11.851170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# NOTE: l0_l1_bridge connects L0 candidates to nearest L1 nodes (u->v).\n",
    "#       l1_anchor_bridge does the reverse: for each L1 node v, attach multiple L0 nodes u (v->u) at L0.\n",
    "neighbor_subset = neighbor_subset\n",
    "# === Run L1-anchored L0 bridge (reverse direction: for each L1 node, attach L0 nodes) ===\n",
    "efQuick = [30, 60, 100]\n",
    "for ef in efQuick:\n",
    "    legacy_recs = []\n",
    "    legacy_visits = []\n",
    "    for i in range(min(test.shape[0], 200)):\n",
    "        q = test[i]\n",
    "        res = naive_custom.search(q, K=K_value, efSearch=ef)\n",
    "        internal_idxs = [ix for ix, _ in res]\n",
    "        orig_idxs = map_internal_to_orig(internal_idxs, naive_internal_to_orig)\n",
    "        legacy_recs.append(calculate_recall(neighbor_subset[i], orig_idxs, k=K_value))\n",
    "        legacy_visits.append(last_or_zero(naive_custom.visited_per_hop))\n",
    "    print(\n",
    "        f\"[custom-naive before layer0 repair] ef={ef}: recall@10={np.mean(legacy_recs):.4f}, visited={np.mean(legacy_visits):.1f}\")\n",
    "\n",
    "cloned_hnsw = clone_hnsw(naive_custom)\n",
    "adds_cnt = l1_anchor_bridge(\n",
    "    cloned_hnsw,\n",
    "    lid_k=15, indeg_quant=0.20,\n",
    "    candidate_prop=0.20,  # use top 30% of L0 by LID among low indegree\n",
    "    per_l1_limit=3,\n",
    "    verbose=True\n",
    ")\n",
    "print(\"[L1-anchored L0 bridge] adds applied:\", adds_cnt)\n",
    "\n",
    "# quick eval\n",
    "for ef in [30, 60, 100]:\n",
    "    recs, visits = [], []\n",
    "    Nq = min(test.shape[0], 200)\n",
    "    for i in range(Nq):\n",
    "        q = test[i]\n",
    "        if distance_method == 'cosine':\n",
    "            q = q / (np.linalg.norm(q) + 1e-12)\n",
    "        out = cloned_hnsw.search(q, K=K_value, efSearch=ef)\n",
    "        internal_idxs = [ix for ix, _ in out]\n",
    "        orig_idxs = map_internal_to_orig(internal_idxs, naive_internal_to_orig)\n",
    "        recs.append(calculate_recall(neighbor_subset[i], orig_idxs, k=K_value))\n",
    "        visits.append(last_or_zero(cloned_hnsw.visited_per_hop))\n",
    "    print(f\"[L1-anchored L0 bridge eval] ef={ef}: recall@10={np.mean(recs):.4f}, visited={np.mean(visits):.1f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom-naive before layer0 repair] ef=30: recall@10=0.7615, visited=776.9\n",
      "[custom-naive before layer0 repair] ef=60: recall@10=0.8275, visited=1336.4\n",
      "[custom-naive before layer0 repair] ef=100: recall@10=0.8560, visited=2039.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3459813/3343454452.py:351: RuntimeWarning: invalid value encountered in log\n",
      "  s = float(np.log(r_k / (dk + 1e-12)).mean())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[L1-anchored] L1_nodes=6174, pool_L0=20000, adds=18522, indeg_thr=32.0\n",
      "[L1-anchored L0 bridge] adds applied: 18522\n",
      "[L1-anchored L0 bridge eval] ef=30: recall@10=0.7655, visited=775.7\n",
      "[L1-anchored L0 bridge eval] ef=60: recall@10=0.8260, visited=1336.0\n",
      "[L1-anchored L0 bridge eval] ef=100: recall@10=0.8505, visited=2021.2\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T07:19:45.900628Z",
     "start_time": "2025-11-04T07:19:21.918235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "ULBAR (Upper-Layer Bridge Audit & Repair):\n",
    "\t•\tTOP layer 그래프 생성 → 커뮤니티 분할\n",
    "\t•\tentry point를 상층의 최대 허브로 재설정\n",
    "\t•\tisolated node 탐지: coverage<2 또는 inter-cluster<0.25\n",
    "\t•\tbad long edges: inter-comm ∧ 길이 quantile 상위(0.7) ∧ redundancy(Jaccard)>0.5\n",
    "\t•\tfallback: bad_edges가 없으면 intra-comm 장거리 상위 일부 drop\n",
    "\t•\tADD proposals: isolated 상위 10개 × 가까운 타 커뮤니티 3개 × 허브 3개\n",
    "\t•\t스왑 적용(drop/add): drop_undirected_edge / add_undirected_edge\n",
    "'''\n",
    "# 기존 recall & visited 측정\n",
    "### Quick eval (custom HNSW naive_custom) — recall & visited\n",
    "efQuick = [30, 60, 100]\n",
    "for ef in efQuick:\n",
    "    legacy_recs = []\n",
    "    legacy_visits = []\n",
    "    for i in range(min(test.shape[0], 200)):\n",
    "        q = test[i]\n",
    "        res = naive_custom.search(q, K=K_value, efSearch=ef)\n",
    "        internal_idxs = [ix for ix, _ in res]\n",
    "        orig_idxs = map_internal_to_orig(internal_idxs, naive_internal_to_orig)\n",
    "        legacy_recs.append(calculate_recall(neighbor_subset[i], orig_idxs, k=K_value))\n",
    "        legacy_visits.append(last_or_zero(naive_custom.visited_per_hop))\n",
    "    print(\n",
    "        f\"[custom-naive before TOP repair] ef={ef}: recall@10={np.mean(legacy_recs):.4f}, visited={np.mean(legacy_visits):.1f}\")\n",
    "# print total node counts\n",
    "for i in range(max(naive_custom.max_level_map().values()), 0, -1):\n",
    "    nbrs = naive_custom._graphs[i]\n",
    "    total_nodes = len(nbrs)\n",
    "    print(f\"[ULBAR before] total nodes at L{i}: {total_nodes}\")\n",
    "\n",
    "# === Run multi-layer ULBAR (top half by default) ===\n",
    "# cloned_hnsw = clone_hnsw(naive_custom)\n",
    "# ops = ulbar_multi_levels(\n",
    "#     cloned_hnsw,\n",
    "#     mode='all',\n",
    "#     q_quant=0.9,\n",
    "#     budgets=(10, 10000),\n",
    "#     jaccard_thresh=0.9,\n",
    "#     C_min=2, r_min=0.1,\n",
    "#     lid_k=15, lid_prop=0.20, indeg_quant=0.30,\n",
    "#     layer0_add_only=True,\n",
    "#     verbose=True\n",
    "# )\n",
    "# print(\"[ULBAR multi] ops summary:\", ops)\n",
    "\n",
    "# === Run L0-only bridge  ===\n",
    "cloned_hnsw = clone_hnsw(naive_custom)\n",
    "ops = ulbar_multi_levels(\n",
    "    cloned_hnsw,\n",
    "    mode='only_L0',\n",
    "    q_quant=0.9,\n",
    "    budgets=(0, 500),              # no drops, only adds on L0\n",
    "    jaccard_thresh=0.9,\n",
    "    C_min=2, r_min=0.1,\n",
    "    lid_k=15, lid_prop=0.10, indeg_quant=0.30,\n",
    "    layer0_add_only=True,\n",
    "    verbose=True\n",
    ")\n",
    "print(\"[ULBAR L0-only] ops summary:\", ops)\n",
    "\n",
    "# Verify whether all nodes in upper layer meets degree cap M, that is, no node has more than M neighbors\n",
    "lvl_map = cloned_hnsw.max_level_map()\n",
    "Lmax = max(lvl_map.values())\n",
    "start = Lmax\n",
    "for layer in range(start, 0, -1):\n",
    "    lc_after = cloned_hnsw._graphs[layer]\n",
    "    violations = []\n",
    "    for u, nbrs in lc_after.items():\n",
    "        deg_u = len(nbrs)\n",
    "        if deg_u > cloned_hnsw._M:\n",
    "            violations.append((u, deg_u))\n",
    "    if violations:\n",
    "        print(f\"[ULBAR] degree cap violations@L{layer}: {len(violations)} (examples) ->\", violations[:5])\n",
    "        throw_error(\"M Degree cap violations detected after ULBAR!\")\n",
    "    else:\n",
    "        print(f\"[ULBAR] all nodes meet degree cap@L{layer} (M={cloned_hnsw._M})\")\n",
    "\n",
    "### Quick eval (custom HNSW naive_custom) — recall & visited (after TOP repair)\n",
    "efQuick = [30, 60, 100]\n",
    "for ef in efQuick:\n",
    "    recs = []\n",
    "    visits = []\n",
    "    for i in range(min(test.shape[0], 200)):\n",
    "        q = test[i]\n",
    "        if distance_method == 'cosine':\n",
    "            q = q / (np.linalg.norm(q) + 1e-12)\n",
    "        res = cloned_hnsw.search(q, K=K_value, efSearch=ef)\n",
    "        internal_idxs = [ix for ix, _ in res]\n",
    "        orig_idxs = map_internal_to_orig(internal_idxs, naive_internal_to_orig)\n",
    "        recs.append(calculate_recall(neighbor_subset[i], orig_idxs, k=K_value))\n",
    "        visits.append(last_or_zero(cloned_hnsw.visited_per_hop))\n",
    "    print(f\"[custom-naive TOP repair] ef={ef}: recall@10={np.mean(recs):.4f}, visited={np.mean(visits):.1f}\")\n",
    "\n",
    "for L in range(max(cloned_hnsw.max_level_map().values()), 0, -1):\n",
    "    def neighbor_set(lc):\n",
    "        return {u: set(nbrs.keys()) for u, nbrs in lc.items()}\n",
    "    before = neighbor_set(naive_custom._graphs[L])\n",
    "    after = neighbor_set(cloned_hnsw._graphs[L])\n",
    "    changed = sum(1 for u in before if before.get(u, set()) != after.get(u, set()))\n",
    "    print(f\"changed nodes@L{L}:\", changed)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom-naive before TOP repair] ef=30: recall@10=0.5370, visited=229.2\n",
      "[custom-naive before TOP repair] ef=60: recall@10=0.6510, visited=383.1\n",
      "[custom-naive before TOP repair] ef=100: recall@10=0.7345, visited=575.8\n",
      "[ULBAR before] total nodes at L8: 1\n",
      "[ULBAR before] total nodes at L7: 4\n",
      "[ULBAR before] total nodes at L6: 16\n",
      "[ULBAR before] total nodes at L5: 51\n",
      "[ULBAR before] total nodes at L4: 199\n",
      "[ULBAR before] total nodes at L3: 754\n",
      "[ULBAR before] total nodes at L2: 3039\n",
      "[ULBAR before] total nodes at L1: 12336\n",
      "[ULBAR multi] target levels: [8, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "[ULBAR L8] detected 1 communities (KMeans, k=1)\n",
      "[ULBAR L8] LID-top∩small-indegree candidates: 1 (prop=0.200, k=15\n",
      "[ULBAR L8] cap-enforced swaps: drop=0, add=0\n",
      "[ULBAR L7] detected 2 communities (KMeans, k=2)\n",
      "[ULBAR L7] LID-top∩small-indegree candidates: 1 (prop=0.200, k=15\n",
      "[ULBAR L7] cap-enforced swaps: drop=0, add=2\n",
      "[ULBAR L6] detected 4 communities (KMeans, k=4)\n",
      "[ULBAR L6] LID-top∩small-indegree candidates: 3 (prop=0.200, k=15\n",
      "[ULBAR L6] cap-enforced swaps: drop=0, add=18\n",
      "[ULBAR L5] detected 7 communities (KMeans, k=7)\n",
      "[ULBAR L5] LID-top∩small-indegree candidates: 10 (prop=0.200, k=15\n",
      "[ULBAR L5] cap-enforced swaps: drop=0, add=87\n",
      "[ULBAR L4] detected 14 communities (KMeans, k=14)\n",
      "[ULBAR L4] LID-top∩small-indegree candidates: 39 (prop=0.200, k=15\n",
      "[ULBAR L4] cap-enforced swaps: drop=0, add=351\n",
      "[ULBAR L3] detected 27 communities (KMeans, k=27)\n",
      "[ULBAR L3] LID-top∩small-indegree candidates: 150 (prop=0.200, k=15\n",
      "[ULBAR L3] cap-enforced swaps: drop=0, add=1350\n",
      "[ULBAR L2] detected 55 communities (KMeans, k=55)\n",
      "[ULBAR L2] LID-top∩small-indegree candidates: 607 (prop=0.200, k=15\n",
      "[ULBAR L2] cap-enforced swaps: drop=0, add=5463\n",
      "[ULBAR L1] detected 111 communities (KMeans, k=111)\n",
      "[ULBAR L1] LID-top∩small-indegree candidates: 2467 (prop=0.200, k=15\n",
      "[ULBAR L1] cap-enforced swaps: drop=0, add=10000\n",
      "[ULBAR L0] detected 128 communities (KMeans, k=128)\n",
      "[ULBAR L0] LID-top∩small-indegree candidates: 10000 (prop=0.200, k=15\n",
      "[ULBAR L0] cap-enforced swaps: drop=0, add=10000\n",
      "[ULBAR multi] ops summary: [(8, 0, 0), (7, 0, 2), (6, 0, 18), (5, 0, 87), (4, 0, 351), (3, 0, 1350), (2, 0, 5463), (1, 0, 10000), (0, 0, 10000)]\n",
      "[ULBAR] all nodes meet degree cap@L8 (M=4)\n",
      "[ULBAR] all nodes meet degree cap@L7 (M=4)\n",
      "[ULBAR] all nodes meet degree cap@L6 (M=4)\n",
      "[ULBAR] all nodes meet degree cap@L5 (M=4)\n",
      "[ULBAR] all nodes meet degree cap@L4 (M=4)\n",
      "[ULBAR] all nodes meet degree cap@L3 (M=4)\n",
      "[ULBAR] all nodes meet degree cap@L2 (M=4)\n",
      "[ULBAR] all nodes meet degree cap@L1 (M=4)\n",
      "[custom-naive TOP repair] ef=30: recall@10=0.5430, visited=230.9\n",
      "[custom-naive TOP repair] ef=60: recall@10=0.6560, visited=381.8\n",
      "[custom-naive TOP repair] ef=100: recall@10=0.7360, visited=571.1\n",
      "changed nodes@L8: 0\n",
      "changed nodes@L7: 0\n",
      "changed nodes@L6: 0\n",
      "changed nodes@L5: 9\n",
      "changed nodes@L4: 13\n",
      "changed nodes@L3: 47\n",
      "changed nodes@L2: 114\n",
      "changed nodes@L1: 476\n"
     ]
    }
   ],
   "execution_count": 253
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T02:25:55.492782Z",
     "start_time": "2025-11-04T02:25:55.488356Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cross-validation-like tuning for ULBAR (no mutation of base index)"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T00:21:16.234269Z",
     "start_time": "2025-11-03T00:20:42.098074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "def eval_index(index: HNSW, ef_list=(60, 100), max_queries=200):\n",
    "    \"\"\"Return dict with mean recall@10 and visited for ef in ef_list.\"\"\"\n",
    "    res = {}\n",
    "    Nq = min(test.shape[0], max_queries)\n",
    "    for ef in ef_list:\n",
    "        recs = []\n",
    "        visits = []\n",
    "        for i in range(Nq):\n",
    "            q = test[i]\n",
    "            if distance_method == 'cosine':\n",
    "                q = q / (np.linalg.norm(q) + 1e-12)\n",
    "            out = index.search(q, K=K_value, efSearch=ef)\n",
    "            internal_idxs = [ix for ix, _ in out]\n",
    "            orig_idxs = map_internal_to_orig(internal_idxs, naive_internal_to_orig)\n",
    "            recs.append(calculate_recall(neighbor_subset[i], orig_idxs, k=K_value))\n",
    "            visits.append(last_or_zero(index.visited_per_hop))\n",
    "        res[(ef, 'recall')] = float(np.mean(recs))\n",
    "        res[(ef, 'visited')] = float(np.mean(visits))\n",
    "    return res\n",
    "\n",
    "\n",
    "def run_ulbar_on_clone(base_index: HNSW, mode='top_half', q_quant=0.7, budgets=(10, 10), jaccard=0.5):\n",
    "    \"\"\"Clone, run ULBAR on clone, return (clone, ops).\"\"\"\n",
    "    idx_clone = clone_hnsw(base_index)\n",
    "    ops = ulbar_multi_levels(idx_clone, mode=mode, q_quant=q_quant, budgets=budgets, jaccard_thresh=jaccard,\n",
    "                             verbose=False)\n",
    "    return idx_clone, ops\n",
    "\n",
    "\n",
    "# 기존 recall & visited 측정\n",
    "### Quick eval (custom HNSW naive_custom) — recall & visited\n",
    "efQuick = [60, 100]\n",
    "for ef in efQuick:\n",
    "    legacy_recs = []\n",
    "    legacy_visits = []\n",
    "    for i in range(min(test.shape[0], 200)):\n",
    "        q = test[i]\n",
    "        res = naive_custom.search(q, K=K_value, efSearch=ef)\n",
    "        internal_idxs = [ix for ix, _ in res]\n",
    "        orig_idxs = map_internal_to_orig(internal_idxs, naive_internal_to_orig)\n",
    "        legacy_recs.append(calculate_recall(neighbor_subset[i], orig_idxs, k=K_value))\n",
    "        legacy_visits.append(last_or_zero(naive_custom.visited_per_hop))\n",
    "    print(\n",
    "        f\"[custom-naive before TOP repair] ef={ef}: recall@10={np.mean(legacy_recs):.4f}, visited={np.mean(legacy_visits):.1f}\")\n",
    "\n",
    "# Parameter grid (compact)\n",
    "modes = ['top_half', 'all_upper']\n",
    "q_quants = [0.6, 0.7]\n",
    "budgetses = [(10, 10), (12, 12)]\n",
    "jaccard_threshold = [0.5, 0.6]\n",
    "\n",
    "grids = list(itertools.product(modes, q_quants, budgetses, jaccard_threshold))\n",
    "\n",
    "results = []\n",
    "best = None\n",
    "best_score = -1e9\n",
    "best_index_clone = None\n",
    "\n",
    "print(f\"[ULBAR CV] trying {len(grids)} combos (won't mutate base index)\")\n",
    "for mode, qv, buds, jaccard_threshold in grids:\n",
    "    idx_try, ops = run_ulbar_on_clone(naive_custom, mode=mode, q_quant=qv, budgets=buds, jaccard=jaccard_threshold)\n",
    "    metrics = eval_index(idx_try, ef_list=(60, 100), max_queries=200)\n",
    "    rec60 = metrics[(60, 'recall')];\n",
    "    rec100 = metrics[(100, 'recall')]\n",
    "    vis60 = metrics[(60, 'visited')];\n",
    "    vis100 = metrics[(100, 'visited')]\n",
    "    # score: prefer recall, penalize visited lightly\n",
    "    score = 0.5 * rec60 + 0.5 * rec100 - 0.001 * ((vis60 + vis100) / 2.0)\n",
    "    results.append({\n",
    "        'mode': mode, 'q_quant': qv, 'budgets': buds,\n",
    "        'jaccard': jaccard_threshold,\n",
    "        'rec60': rec60, 'rec100': rec100,\n",
    "        'vis60': vis60, 'vis100': vis100,\n",
    "        'score': score,\n",
    "        'ops': ops,\n",
    "    })\n",
    "    print(\n",
    "        f\"  {mode:9s} q={qv:.1f} buds={buds} jaccard={jaccard_threshold}| rec60={rec60:.4f}, rec100={rec100:.4f}, vis={((vis60 + vis100) / 2):.1f}, score={score:.5f}\")\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best = (mode, qv, buds)\n",
    "        best_index_clone = idx_try\n",
    "\n",
    "# Summary\n",
    "print(\"\\n[ULBAR CV] Results summary (sorted by score):\")\n",
    "results_sorted = sorted(results, key=lambda r: r['score'], reverse=True)\n",
    "for r in results_sorted:\n",
    "    print(r)\n",
    "\n",
    "print(\"\\n[ULBAR CV] Best config:\", best, \"score=\", best_score)\n",
    "# best_index_clone holds the best modified copy; naive_custom remains unmodified"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom-naive before TOP repair] ef=60: recall@10=0.7995, visited=921.6\n",
      "[custom-naive before TOP repair] ef=100: recall@10=0.8470, visited=1341.7\n",
      "[ULBAR CV] trying 16 combos (won't mutate base index)\n",
      "  top_half  q=0.6 buds=(10, 10) jaccard=0.5| rec60=0.7995, rec100=0.8470, vis=1131.3, score=-0.30802\n",
      "  top_half  q=0.6 buds=(10, 10) jaccard=0.6| rec60=0.7995, rec100=0.8470, vis=1131.2, score=-0.30799\n",
      "  top_half  q=0.6 buds=(12, 12) jaccard=0.5| rec60=0.7995, rec100=0.8470, vis=1131.3, score=-0.30802\n",
      "  top_half  q=0.6 buds=(12, 12) jaccard=0.6| rec60=0.7995, rec100=0.8470, vis=1131.2, score=-0.30799\n",
      "  top_half  q=0.7 buds=(10, 10) jaccard=0.5| rec60=0.7995, rec100=0.8470, vis=1131.2, score=-0.30799\n",
      "  top_half  q=0.7 buds=(10, 10) jaccard=0.6| rec60=0.7995, rec100=0.8470, vis=1131.2, score=-0.30799\n",
      "  top_half  q=0.7 buds=(12, 12) jaccard=0.5| rec60=0.7995, rec100=0.8470, vis=1131.2, score=-0.30799\n",
      "  top_half  q=0.7 buds=(12, 12) jaccard=0.6| rec60=0.7995, rec100=0.8470, vis=1131.2, score=-0.30799\n",
      "  all_upper q=0.6 buds=(10, 10) jaccard=0.5| rec60=0.7995, rec100=0.8470, vis=1131.3, score=-0.30802\n",
      "  all_upper q=0.6 buds=(10, 10) jaccard=0.6| rec60=0.7995, rec100=0.8470, vis=1131.2, score=-0.30799\n",
      "  all_upper q=0.6 buds=(12, 12) jaccard=0.5| rec60=0.7995, rec100=0.8470, vis=1131.3, score=-0.30802\n",
      "  all_upper q=0.6 buds=(12, 12) jaccard=0.6| rec60=0.7995, rec100=0.8470, vis=1131.2, score=-0.30799\n",
      "  all_upper q=0.7 buds=(10, 10) jaccard=0.5| rec60=0.7995, rec100=0.8470, vis=1131.2, score=-0.30799\n",
      "  all_upper q=0.7 buds=(10, 10) jaccard=0.6| rec60=0.7995, rec100=0.8470, vis=1131.2, score=-0.30799\n",
      "  all_upper q=0.7 buds=(12, 12) jaccard=0.5| rec60=0.7995, rec100=0.8470, vis=1131.2, score=-0.30799\n",
      "  all_upper q=0.7 buds=(12, 12) jaccard=0.6| rec60=0.7995, rec100=0.8470, vis=1131.2, score=-0.30799\n",
      "\n",
      "[ULBAR CV] Results summary (sorted by score):\n",
      "{'mode': 'top_half', 'q_quant': 0.6, 'budgets': (10, 10), 'jaccard': 0.6, 'rec60': 0.7995, 'rec100': 0.8470000000000002, 'vis60': 921.025, 'vis100': 1341.455, 'score': -0.30799, 'ops': [(3, 1, 0), (2, 10, 0), (1, 10, 10)]}\n",
      "{'mode': 'top_half', 'q_quant': 0.6, 'budgets': (12, 12), 'jaccard': 0.6, 'rec60': 0.7995, 'rec100': 0.8470000000000002, 'vis60': 921.025, 'vis100': 1341.455, 'score': -0.30799, 'ops': [(3, 1, 0), (2, 12, 0), (1, 12, 12)]}\n",
      "{'mode': 'top_half', 'q_quant': 0.7, 'budgets': (10, 10), 'jaccard': 0.5, 'rec60': 0.7995, 'rec100': 0.8470000000000002, 'vis60': 921.025, 'vis100': 1341.455, 'score': -0.30799, 'ops': [(3, 1, 0), (2, 1, 0), (1, 10, 10)]}\n",
      "{'mode': 'top_half', 'q_quant': 0.7, 'budgets': (10, 10), 'jaccard': 0.6, 'rec60': 0.7995, 'rec100': 0.8470000000000002, 'vis60': 921.025, 'vis100': 1341.455, 'score': -0.30799, 'ops': [(3, 1, 0), (2, 10, 0), (1, 10, 10)]}\n",
      "{'mode': 'top_half', 'q_quant': 0.7, 'budgets': (12, 12), 'jaccard': 0.5, 'rec60': 0.7995, 'rec100': 0.8470000000000002, 'vis60': 921.025, 'vis100': 1341.455, 'score': -0.30799, 'ops': [(3, 1, 0), (2, 1, 0), (1, 12, 12)]}\n",
      "{'mode': 'top_half', 'q_quant': 0.7, 'budgets': (12, 12), 'jaccard': 0.6, 'rec60': 0.7995, 'rec100': 0.8470000000000002, 'vis60': 921.025, 'vis100': 1341.455, 'score': -0.30799, 'ops': [(3, 1, 0), (2, 12, 0), (1, 12, 12)]}\n",
      "{'mode': 'all_upper', 'q_quant': 0.6, 'budgets': (10, 10), 'jaccard': 0.6, 'rec60': 0.7995, 'rec100': 0.8470000000000002, 'vis60': 921.025, 'vis100': 1341.455, 'score': -0.30799, 'ops': [(3, 1, 0), (2, 10, 0), (1, 10, 10)]}\n",
      "{'mode': 'all_upper', 'q_quant': 0.6, 'budgets': (12, 12), 'jaccard': 0.6, 'rec60': 0.7995, 'rec100': 0.8470000000000002, 'vis60': 921.025, 'vis100': 1341.455, 'score': -0.30799, 'ops': [(3, 1, 0), (2, 12, 0), (1, 12, 12)]}\n",
      "{'mode': 'all_upper', 'q_quant': 0.7, 'budgets': (10, 10), 'jaccard': 0.5, 'rec60': 0.7995, 'rec100': 0.8470000000000002, 'vis60': 921.025, 'vis100': 1341.455, 'score': -0.30799, 'ops': [(3, 1, 0), (2, 1, 0), (1, 10, 10)]}\n",
      "{'mode': 'all_upper', 'q_quant': 0.7, 'budgets': (10, 10), 'jaccard': 0.6, 'rec60': 0.7995, 'rec100': 0.8470000000000002, 'vis60': 921.025, 'vis100': 1341.455, 'score': -0.30799, 'ops': [(3, 1, 0), (2, 10, 0), (1, 10, 10)]}\n",
      "{'mode': 'all_upper', 'q_quant': 0.7, 'budgets': (12, 12), 'jaccard': 0.5, 'rec60': 0.7995, 'rec100': 0.8470000000000002, 'vis60': 921.025, 'vis100': 1341.455, 'score': -0.30799, 'ops': [(3, 1, 0), (2, 1, 0), (1, 12, 12)]}\n",
      "{'mode': 'all_upper', 'q_quant': 0.7, 'budgets': (12, 12), 'jaccard': 0.6, 'rec60': 0.7995, 'rec100': 0.8470000000000002, 'vis60': 921.025, 'vis100': 1341.455, 'score': -0.30799, 'ops': [(3, 1, 0), (2, 12, 0), (1, 12, 12)]}\n",
      "{'mode': 'top_half', 'q_quant': 0.6, 'budgets': (10, 10), 'jaccard': 0.5, 'rec60': 0.7995, 'rec100': 0.8470000000000002, 'vis60': 921.04, 'vis100': 1341.5, 'score': -0.30801999999999996, 'ops': [(3, 1, 0), (2, 2, 0), (1, 10, 10)]}\n",
      "{'mode': 'top_half', 'q_quant': 0.6, 'budgets': (12, 12), 'jaccard': 0.5, 'rec60': 0.7995, 'rec100': 0.8470000000000002, 'vis60': 921.04, 'vis100': 1341.5, 'score': -0.30801999999999996, 'ops': [(3, 1, 0), (2, 2, 0), (1, 12, 12)]}\n",
      "{'mode': 'all_upper', 'q_quant': 0.6, 'budgets': (10, 10), 'jaccard': 0.5, 'rec60': 0.7995, 'rec100': 0.8470000000000002, 'vis60': 921.04, 'vis100': 1341.5, 'score': -0.30801999999999996, 'ops': [(3, 1, 0), (2, 2, 0), (1, 10, 10)]}\n",
      "{'mode': 'all_upper', 'q_quant': 0.6, 'budgets': (12, 12), 'jaccard': 0.5, 'rec60': 0.7995, 'rec100': 0.8470000000000002, 'vis60': 921.04, 'vis100': 1341.5, 'score': -0.30801999999999996, 'ops': [(3, 1, 0), (2, 2, 0), (1, 12, 12)]}\n",
      "\n",
      "[ULBAR CV] Best config: ('top_half', 0.6, (10, 10)) score= -0.30799\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "HUSjgR9F8F8R",
    "zv8ixRkm-vfy",
    "C6bBcvRinjxR"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
