{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Upper layer의 long bridge 조작 - trial1 w/ faiss"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Preparation"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T05:59:49.522775Z",
     "start_time": "2025-10-30T05:58:43.690634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import faiss  # hnswlib 대신 faiss 임포트\n",
    "import numpy as np\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "from sklearn.cluster import KMeans\n",
    "import h5py\n",
    "import struct\n",
    "from collections import defaultdict  # cluster-wise order에 필요\n",
    "\n",
    "def read_fvecs(filename):\n",
    "    \"\"\"Reads .fvecs binary file into np.ndarray of shape (n, d).\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = f.read()\n",
    "    dim = struct.unpack('i', data[:4])[0]\n",
    "    vecs = np.frombuffer(data, dtype=np.float32)\n",
    "    vecs = vecs.reshape(-1, dim + 1)[:, 1:]  # drop the leading 'dim'\n",
    "    return vecs\n",
    "\n",
    "def read_ivecs(filename):\n",
    "    \"\"\"Reads .ivecs binary file into np.ndarray of shape (n, k).\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = f.read()\n",
    "    dim = struct.unpack('i', data[:4])[0]\n",
    "    vecs = np.frombuffer(data, dtype=np.int32)\n",
    "    vecs = vecs.reshape(-1, dim + 1)[:, 1:]\n",
    "    return vecs\n",
    "\n",
    "# 데이터셋 경로 (현재 구조에 맞춰 수정)\n",
    "base_path = \"./datasets\"\n",
    "# 데이터셋 경로\n",
    "file_path = base_path + \"/glove-200-angular.hdf5\"\n",
    "\n",
    "# h5py를 사용하여 파일 열기\n",
    "with h5py.File(file_path, 'r') as f:\n",
    "    # HDF5 파일 내의 데이터셋 키 확인 (어떤 데이터가 있는지 모를 경우 유용)\n",
    "    print(f\"Keys in HDF5 file: {list(f.keys())}\")\n",
    "\n",
    "    # 각 데이터셋을 numpy 배열로 불러오기\n",
    "    train = np.array(f['train'])\n",
    "    test = np.array(f['test'])\n",
    "    neighbors = np.array(f['neighbors'])\n",
    "    # distances 데이터셋이 있다면 같이 로드할 수 있습니다.\n",
    "    # distances = np.array(f['distances'])\n",
    "\n",
    "# random sample 100,000 from train\n",
    "seed = 42\n",
    "n_target = 100_000\n",
    "rng = np.random.RandomState(seed)\n",
    "idx = rng.choice(train.shape[0], n_target, replace=False)\n",
    "train = train[idx]\n",
    "\n",
    "dim = train.shape[1]\n",
    "efConstruction = 100\n",
    "paramM = 16\n",
    "distance_method = 'cosine'\n",
    "\n",
    "# --- 3. Naive vs Cluster-wise: 두 인덱스 빌드 & 시각화 ---\n",
    "print(\"Building Faiss HNSW (naive vs cluster-wise) ...\")\n",
    "\n",
    "# (1) Cosine ~= Inner Product 를 위해 L2 정규화\n",
    "train_norm = train.copy()\n",
    "faiss.normalize_L2(train_norm)\n",
    "\n",
    "# (2) 하나의 공통 UMAP 레이아웃(공정 비교)\n",
    "print(\"Running UMAP for 2D layout...\")\n",
    "reducer = umap.UMAP(\n",
    "    n_components=2,\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.1,\n",
    "    metric=distance_method,   # 'cosine'\n",
    "    random_state=42\n",
    ")\n",
    "embedding_2d = reducer.fit_transform(train_norm)  # (N, 2)\n",
    "\n",
    "def build_faiss_hnsw_index(vectors, order, M, efc):\n",
    "    \"\"\"지정한 삽입 순서(order)로 HNSW 인덱스를 빌드.\"\"\"\n",
    "    index = faiss.IndexHNSWFlat(dim, M, faiss.METRIC_INNER_PRODUCT)\n",
    "    index.hnsw.efConstruction = efc\n",
    "    index.add(vectors[order])  # 순서대로 add\n",
    "    return index\n",
    "\n",
    "# 쿼리를 아예 제외하고 생각해야함\n",
    "def extract_layer0_edges(hnsw_index):\n",
    "    \"\"\"Faiss HNSW의 layer-0 엣지(인접리스트) 추출.\"\"\"\n",
    "    offsets = faiss.vector_to_array(hnsw_index.hnsw.offsets)\n",
    "    neighbors_flat = faiss.vector_to_array(hnsw_index.hnsw.neighbors)\n",
    "\n",
    "    edges = []\n",
    "    for i in range(len(offsets) - 1):\n",
    "        s, e = offsets[i], offsets[i + 1]\n",
    "        for nb in neighbors_flat[s:e]:\n",
    "            if nb >= 0:\n",
    "                edges.append((i, int(nb)))\n",
    "    return edges\n",
    "\n",
    "def to_edge_set_in_original_ids(edges, order):\n",
    "    \"\"\"\n",
    "    hnsw 내부 id로 표현된 edges 를 '원본 샘플 id' 쌍(무방향, 정렬된 튜플)으로 변환해 set으로 반환.\n",
    "    - edges: [(u_internal, v_internal), ...]\n",
    "    - order: 내부 id -> 원본 id 로의 매핑을 담은 배열(naive_order / clustered_order)\n",
    "    \"\"\"\n",
    "    order = np.asarray(order)\n",
    "    s = set()\n",
    "    for u, v in edges:\n",
    "        ou = int(order[u])\n",
    "        ov = int(order[v])\n",
    "        if ou == ov:\n",
    "            continue\n",
    "        # 무방향(edge 중복 제거)을 위해 정렬된 튜플로 저장\n",
    "        if ou < ov:\n",
    "            s.add((ou, ov))\n",
    "        else:\n",
    "            s.add((ov, ou))\n",
    "    return s\n",
    "\n",
    "def filter_by_umap_length(edge_set, coords, threshold=None):\n",
    "    \"\"\"\n",
    "    UMAP 상에서 일정 길이(threshold) 이상인 엣지만 남기고 싶을 때 사용.\n",
    "    threshold가 None이면 필터링 하지 않음.\n",
    "    - edge_set: {(u_orig, v_orig), ...}  (원본 id 공간)\n",
    "    - coords: UMAP 좌표 (원본 id 순서의 (N, 2) 배열)\n",
    "    \"\"\"\n",
    "    if threshold is None:\n",
    "        return edge_set\n",
    "    out = set()\n",
    "    for u, v in edge_set:\n",
    "        if np.linalg.norm(coords[u] - coords[v]) >= threshold:\n",
    "            out.add((u, v))\n",
    "    return out\n",
    "\n",
    "# (3-A) Naive(랜덤 삽입 순서)\n",
    "rng_vis = np.random.RandomState(42)\n",
    "naive_order = rng_vis.permutation(n_target)\n",
    "\n",
    "index_naive = build_faiss_hnsw_index(train_norm, naive_order, paramM, efConstruction)\n",
    "edges_naive = extract_layer0_edges(index_naive)\n",
    "coords_naive = embedding_2d[naive_order]  # 인덱스의 내부 id와 좌표 정렬을 맞춤\n",
    "\n",
    "# (3-B) Cluster-wise(클러스터 순서로 삽입)\n",
    "kmeans = KMeans(n_clusters=100, n_init='auto', random_state=21).fit(train_norm)\n",
    "cluster_data = defaultdict(list)\n",
    "for i, lbl in enumerate(kmeans.labels_):\n",
    "    cluster_data[int(lbl)].append(i)\n",
    "\n",
    "clustered_order = []\n",
    "for c in sorted(cluster_data.keys()):\n",
    "    clustered_order.extend(cluster_data[c])\n",
    "\n",
    "index_clustered = build_faiss_hnsw_index(train_norm, clustered_order, paramM, efConstruction)\n",
    "edges_clustered = extract_layer0_edges(index_clustered)\n",
    "coords_clustered = embedding_2d[clustered_order]\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in HDF5 file: ['distances', 'neighbors', 'test', 'train']\n",
      "Building Faiss HNSW (naive vs cluster-wise) ...\n",
      "Running UMAP for 2D layout...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/venv/lib/python3.8/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Upper-layer Long-Bridge Audit & Repair (ULBAR) – scaffolding"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T05:56:59.141211Z",
     "start_time": "2025-10-30T05:56:58.692873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "\n",
    "# --------------- FAISS HNSW helpers (levels & overlay) ---------------\n",
    "\n",
    "def get_hnsw_levels(hnsw_index):\n",
    "    \"\"\"Return numpy array of per-node max level (0-based).\"\"\"\n",
    "    return faiss.vector_to_array(hnsw_index.hnsw.levels)\n",
    "\n",
    "levels_naive     = get_hnsw_levels(index_naive)\n",
    "levels_clustered = get_hnsw_levels(index_clustered)\n",
    "\n",
    "print(\"Max level (naive, clustered):\", levels_naive.max(), levels_clustered.max())\n",
    "\n",
    "# Build an overlay graph among nodes whose level >= min_level.\n",
    "# Since FAISS Python API doesn't expose per-level adjacency, we approximate an upper-layer graph\n",
    "# by constructing a small kNN graph within the upper-node set using cosine (IP on normalized).\n",
    "\n",
    "def build_upper_overlay_knn(X_norm, node_ids, k=8):\n",
    "    \"\"\"Build a small undirected kNN overlay graph among node_ids.\n",
    "    Returns a NetworkX Graph over original ids and an adjacency dict {u: set(vs)}.\n",
    "    \"\"\"\n",
    "    if len(node_ids) == 0:\n",
    "        G = nx.Graph(); G.add_nodes_from([])\n",
    "        return G, {}\n",
    "    sub = X_norm[node_ids]\n",
    "    index_flat = faiss.IndexFlatIP(sub.shape[1])\n",
    "    index_flat.add(sub)\n",
    "    D, I = index_flat.search(sub, min(k+1, len(node_ids)))  # self included\n",
    "    adj = {int(node_ids[i]): set() for i in range(len(node_ids))}\n",
    "    for i in range(len(node_ids)):\n",
    "        u = int(node_ids[i])\n",
    "        for j in I[i]:\n",
    "            j = int(j)\n",
    "            if j == i:  # skip self\n",
    "                continue\n",
    "            v = int(node_ids[j])\n",
    "            adj[u].add(v)\n",
    "            adj[v].add(u)\n",
    "    G = nx.Graph()\n",
    "    for u, vs in adj.items():\n",
    "        for v in vs:\n",
    "            if u < v:\n",
    "                G.add_edge(u, v)\n",
    "    return G, adj\n",
    "\n",
    "# Choose a target upper layer (e.g., min_level = 2 or max_level-1 if shallow)\n",
    "min_level = int(max(2, int(levels_naive.max()) - 1))\n",
    "upper_nodes_naive     = np.where(levels_naive     >= min_level)[0]\n",
    "upper_nodes_clustered = np.where(levels_clustered >= min_level)[0]\n",
    "print(f\"Upper nodes (>=L{min_level}): naive={len(upper_nodes_naive)}, clustered={len(upper_nodes_clustered)}\")\n",
    "\n",
    "G_up_naive,     adj_up_naive     = build_upper_overlay_knn(train_norm, upper_nodes_naive,     k=8)\n",
    "G_up_clustered, adj_up_clustered = build_upper_overlay_knn(train_norm, upper_nodes_clustered, k=8)\n",
    "\n",
    "# --------------- Community & medoids ---------------\n",
    "\n",
    "def louvain_partition(G):\n",
    "    try:\n",
    "        from networkx.algorithms.community import louvain_communities\n",
    "        comms = louvain_communities(G, seed=42)\n",
    "    except Exception:\n",
    "        from networkx.algorithms.community import greedy_modularity_communities\n",
    "        comms = greedy_modularity_communities(G)\n",
    "    comm_id = {}\n",
    "    for cid, C in enumerate(comms):\n",
    "        for v in C:\n",
    "            comm_id[int(v)] = cid\n",
    "    return comm_id, [set(c) for c in comms]\n",
    "\n",
    "def medoid_of_nodes(nodes, X_norm):\n",
    "    nodes = list(nodes)\n",
    "    if not nodes:\n",
    "        return None\n",
    "    sub = X_norm[nodes]\n",
    "    S = sub @ sub.T\n",
    "    Dsum = (1.0 - S).sum(axis=1)\n",
    "    midx = int(np.argmin(Dsum))\n",
    "    return int(nodes[midx])\n",
    "\n",
    "comm_naive,     comms_naive     = louvain_partition(G_up_naive)\n",
    "comm_clustered, comms_clustered = louvain_partition(G_up_clustered)\n",
    "print(f\"#Communities: naive={len(comms_naive)}, clustered={len(comms_clustered)}\")\n",
    "\n",
    "# --------------- Audit rules ---------------\n",
    "\n",
    "def coverage_and_ratio(u, adj, comm_id):\n",
    "    vs = list(adj.get(u, []))\n",
    "    if not vs:\n",
    "        return 0, 0.0\n",
    "    cov = len({comm_id.get(v, -1) for v in vs})\n",
    "    ic  = sum(1 for v in vs if comm_id.get(v, -1) != comm_id.get(u, -1)) / len(vs)\n",
    "    return cov, ic\n",
    "\n",
    "def cosine_dist(u, v, X_norm):\n",
    "    return 1.0 - float(np.dot(X_norm[u], X_norm[v]))\n",
    "\n",
    "def jaccard(u, v, adj):\n",
    "    a, b = adj.get(u, set()), adj.get(v, set())\n",
    "    if not a and not b:\n",
    "        return 0.0\n",
    "    inter = len(a & b); uni = len(a | b)\n",
    "    return inter / max(uni, 1)\n",
    "\n",
    "# (A) Isolated highway entrance candidates (low coverage/inter-cluster)\n",
    "C_min, r_min = 2, 0.25\n",
    "\n",
    "def find_isolated_nodes(adj, comm_id):\n",
    "    cands = []\n",
    "    for u in adj.keys():\n",
    "        cov, ic = coverage_and_ratio(u, adj, comm_id)\n",
    "        if cov < C_min or ic < r_min:\n",
    "            cands.append((u, cov, ic))\n",
    "    return sorted(cands, key=lambda x: (x[1], x[2]))\n",
    "\n",
    "iso_naive     = find_isolated_nodes(adj_up_naive,     comm_naive)\n",
    "iso_clustered = find_isolated_nodes(adj_up_clustered, comm_clustered)\n",
    "print(f\"Isolated@upper: naive={len(iso_naive)}, clustered={len(iso_clustered)} (show 5)\")\n",
    "print(\"naive examples:\", iso_naive[:5])\n",
    "\n",
    "# (B) Misplaced highway candidates (inter-comm & long & redundant)\n",
    "length_q = 0.7\n",
    "\n",
    "def find_long_edges(adj, X_norm, q=0.7):\n",
    "    # collect all edge lengths\n",
    "    lens = []\n",
    "    seen = set()\n",
    "    for u, vs in adj.items():\n",
    "        for v in vs:\n",
    "            a, b = (u, v) if u < v else (v, u)\n",
    "            if (a, b) in seen: continue\n",
    "            seen.add((a, b))\n",
    "            lens.append(cosine_dist(a, b, X_norm))\n",
    "    lens = np.array(lens) if lens else np.array([0.0])\n",
    "    tau = float(np.quantile(lens, q))\n",
    "    return tau\n",
    "\n",
    "\n",
    "tau_na = find_long_edges(adj_up_naive, train_norm, q=length_q)\n",
    "\n",
    "def bad_edge_candidates(adj, comm_id, X_norm, tau):\n",
    "    bad = []\n",
    "    seen = set()\n",
    "    for u, vs in adj.items():\n",
    "        for v in vs:\n",
    "            a, b = (u, v) if u < v else (v, u)\n",
    "            if (a, b) in seen: continue\n",
    "            seen.add((a, b))\n",
    "            if comm_id.get(a, -1) == comm_id.get(b, -1):\n",
    "                continue\n",
    "            d = cosine_dist(a, b, X_norm)\n",
    "            if d < tau:\n",
    "                continue  # not long enough\n",
    "            jac = jaccard(a, b, adj)\n",
    "            if jac > 0.5:\n",
    "                bad.append((a, b, d, jac))\n",
    "    return sorted(bad, key=lambda x: (-x[3], -x[2]))\n",
    "\n",
    "bad_naive = bad_edge_candidates(adj_up_naive, comm_naive, train_norm, tau_na)\n",
    "print(f\"Bad long-edges (naive): {len(bad_naive)}; examples:\", bad_naive[:5])\n",
    "\n",
    "# (C) Missing-city detection: communities present at layer-0 but absent in upper\n",
    "# For a quick proxy, use k-means on X_norm as 'semantic clusters' (or reuse existing kmeans above if available)\n",
    "K_sem = 50\n",
    "km_sem = KMeans(n_clusters=K_sem, n_init='auto', random_state=123).fit(train_norm)\n",
    "labels_sem = km_sem.labels_\n",
    "\n",
    "upper_presence = np.zeros(K_sem, dtype=bool)\n",
    "for u in upper_nodes_naive:\n",
    "    upper_presence[labels_sem[u]] = True\n",
    "missing_clusters = np.where(~upper_presence)[0].tolist()\n",
    "print(f\"Missing (no upper node) semantic clusters: {len(missing_clusters)} (show 5):\", missing_clusters[:5])\n",
    "\n",
    "# For each missing cluster, pick the medoid (closest to its centroid) as a promotion candidate\n",
    "def cluster_medoid_indices(X_norm, labels, cluster_ids):\n",
    "    c2idx = {cid: np.where(labels == cid)[0] for cid in cluster_ids}\n",
    "    promos = []\n",
    "    for cid, idxs in c2idx.items():\n",
    "        if len(idxs) == 0: continue\n",
    "        sub = X_norm[idxs]\n",
    "        centroid = sub.mean(axis=0)\n",
    "        d = 1.0 - sub @ centroid  # cosine dist to centroid (approx)\n",
    "        m = int(idxs[np.argmin(d)])\n",
    "        promos.append((cid, m))\n",
    "    return promos\n",
    "\n",
    "promote_naive = cluster_medoid_indices(train_norm, labels_sem, missing_clusters)\n",
    "print(\"Promotion candidates (cluster_id, medoid_id) examples:\", promote_naive[:5])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max level (naive, clustered): 7 7\n",
      "Upper nodes (>=L6): naive=7, clustered=7\n",
      "#Communities: naive=1, clustered=1\n",
      "Isolated@upper: naive=7, clustered=7 (show 5)\n",
      "naive examples: [(592, 1, 0.0), (1751, 1, 0.0), (3123, 1, 0.0), (3913, 1, 0.0), (5033, 1, 0.0)]\n",
      "Bad long-edges (naive): 0; examples: []\n",
      "Missing (no upper node) semantic clusters: 43 (show 5): [0, 1, 2, 3, 4]\n",
      "Promotion candidates (cluster_id, medoid_id) examples: [(0, 5426), (1, 7359), (2, 9674), (3, 6944), (4, 8408)]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Overlay proposal (no in-place modification)"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T05:57:16.566508Z",
     "start_time": "2025-10-30T05:57:16.553125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We won't mutate FAISS index. Instead, we propose overlay operations for experimental greedy descent.\n",
    "# Proposed operations:\n",
    "#  - ADD for isolated nodes: connect to top-L nearby communities' hubs/medoids\n",
    "#  - DROP for bad edges\n",
    "\n",
    "# Build per-community hub list on the upper overlay graph (by degree)\n",
    "\n",
    "def top_hubs_by_comm(G, comm_id, top_p=0.1, min_per_comm=3):\n",
    "    deg = dict(G.degree())\n",
    "    per_comm = defaultdict(list)\n",
    "    for v, d in deg.items():\n",
    "        c = comm_id.get(v, -1)\n",
    "        per_comm[c].append((d, v))\n",
    "    hubs = {}\n",
    "    for c, arr in per_comm.items():\n",
    "        arr.sort(reverse=True)\n",
    "        k = max(min_per_comm, int(len(arr) * top_p))\n",
    "        hubs[c] = [v for _, v in arr[:k]]\n",
    "    return hubs\n",
    "\n",
    "hubs_naive = top_hubs_by_comm(G_up_naive, comm_naive, top_p=0.2, min_per_comm=2)\n",
    "\n",
    "# Propose ADDs for the top-10 isolated\n",
    "L_neigh = 3\n",
    "K_per_comm = 3\n",
    "\n",
    "def nearest_communities(node, comm_id, comms, X_norm, L=3):\n",
    "    cu = comm_id.get(node, -1)\n",
    "    # distance between node and other communities' medoids\n",
    "    medoids = {}\n",
    "    for cid, C in enumerate(comms):\n",
    "        m = medoid_of_nodes(C, X_norm)\n",
    "        if m is None: continue\n",
    "        medoids[cid] = m\n",
    "    cand = []\n",
    "    for cid, m in medoids.items():\n",
    "        if cid == cu: continue\n",
    "        d = cosine_dist(node, m, X_norm)\n",
    "        cand.append((d, cid))\n",
    "    cand.sort()\n",
    "    return [cid for _, cid in cand[:L]]\n",
    "\n",
    "add_proposals = []\n",
    "for (u, cov, ic) in iso_naive[:10]:\n",
    "    ncomms = nearest_communities(u, comm_naive, comms_naive, train_norm, L=L_neigh)\n",
    "    for cid in ncomms:\n",
    "        for v in hubs_naive.get(cid, [])[:K_per_comm]:\n",
    "            d = cosine_dist(u, v, train_norm)\n",
    "            jac = jaccard(u, v, adj_up_naive)\n",
    "            add_proposals.append((u, v, d, jac))\n",
    "\n",
    "print(\"ADD proposals (u,v,dist,jacc) examples:\", sorted(add_proposals, key=lambda x: (x[2], x[3]))[:10])\n",
    "\n",
    "# DROP proposals from bad_naive\n",
    "print(\"DROP proposals (a,b,dist,jacc) examples:\", bad_naive[:10])\n",
    "\n",
    "# NOTE: next step would be to build an overlay adjacency by applying a small number of ADD/DROP\n",
    "# and run a custom greedy descent (overlay for upper layers, FAISS for layer-0) to measure recall/visited.\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADD proposals (u,v,dist,jacc) examples: []\n",
      "DROP proposals (a,b,dist,jacc) examples: []\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tiny overlay apply (dry run)"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T05:57:41.201042Z",
     "start_time": "2025-10-30T05:57:41.194514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "overlay_adj = {u: set(vs) for u, vs in adj_up_naive.items()}  # shallow copy\n",
    "\n",
    "# Apply up to 10 DROP and 10 ADD from our proposals\n",
    "for (a,b,dist,jac) in bad_naive[:10]:\n",
    "    if b in overlay_adj.get(a, set()):\n",
    "        overlay_adj[a].discard(b)\n",
    "        overlay_adj.setdefault(b, set()).discard(a)\n",
    "\n",
    "for (u,v,dist,jac) in sorted(add_proposals, key=lambda x: (x[2], -x[3]))[:10]:\n",
    "    overlay_adj.setdefault(u, set()).add(v)\n",
    "    overlay_adj.setdefault(v, set()).add(u)\n",
    "\n",
    "# Report coverage/inter-cluster for a few isolated nodes before/after\n",
    "def coverage_ratio_on(adj, comm_id, nodes):\n",
    "    out = []\n",
    "    for u in nodes:\n",
    "        cov, ic = coverage_and_ratio(u, adj, comm_id)\n",
    "        out.append((u, cov, ic))\n",
    "    return out\n",
    "\n",
    "print(\"Before (first 5 iso):\", coverage_ratio_on(adj_up_naive, comm_naive, [u for u,_,_ in iso_naive[:5]]))\n",
    "print(\"After  (first 5 iso):\", coverage_ratio_on(overlay_adj,  comm_naive, [u for u,_,_ in iso_naive[:5]]))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before (first 5 iso): [(592, 1, 0.0), (1751, 1, 0.0), (3123, 1, 0.0), (3913, 1, 0.0), (5033, 1, 0.0)]\n",
      "After  (first 5 iso): [(592, 1, 0.0), (1751, 1, 0.0), (3123, 1, 0.0), (3913, 1, 0.0), (5033, 1, 0.0)]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "HUSjgR9F8F8R",
    "zv8ixRkm-vfy",
    "C6bBcvRinjxR"
   ]
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
