{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYYCRJHqzS8h"
   },
   "source": [
    "# Topic 1\n",
    "data insertion시 sorting의 영향"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLfHRzb5zWog"
   },
   "source": [
    "## HNSW"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rFkpzTKyy3sv",
    "ExecuteTime": {
     "end_time": "2025-10-14T04:02:57.626654Z",
     "start_time": "2025-10-14T04:02:57.518441Z"
    }
   },
   "source": [
    "## Measuring accuracy and visited nodes per hop in HNSW (to examine the possibility of early termination)\n",
    "\n",
    "# https://github.com/RyanLiGod/hnsw-python/blob/master/hnsw.py\n",
    "\n",
    "from heapq import heapify, heappop, heappush, heapreplace, nlargest, nsmallest\n",
    "from math import log2\n",
    "from operator import itemgetter\n",
    "from random import random\n",
    "import numpy as np\n",
    "from fontTools.ttLib.ttVisitor import visit\n",
    "\n",
    "\n",
    "class HNSW:\n",
    "    # self._graphs[level][i] contains a {j: dist} dictionary,\n",
    "    # where j is a neighbor of i and dist is distance\n",
    "\n",
    "    def l2_distance(self, a, b):\n",
    "        return np.linalg.norm(a - b)\n",
    "\n",
    "    def cosine_distance(self, a, b):\n",
    "        try:\n",
    "            return 1 - np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "        except ValueError:\n",
    "            print(a)\n",
    "            print(b)\n",
    "\n",
    "    def vectorized_distance_(self, x, ys):\n",
    "        return [self.distance_func(x, y) for y in ys]\n",
    "\n",
    "    def __init__(self, distance_type, M=5, efConstruction=200, Mmax=None):\n",
    "        if distance_type == \"l2\":\n",
    "            distance_func = self.l2_distance\n",
    "        elif distance_type == \"cosine\":\n",
    "            distance_func = self.cosine_distance\n",
    "        else:\n",
    "            raise TypeError('Please check your distance type!')\n",
    "        self.distance_func = distance_func\n",
    "        self.vectorized_distance = self.vectorized_distance_\n",
    "        self._M = M\n",
    "        self._efConstruction = efConstruction\n",
    "        self._Mmax = 2 * M if Mmax is None else Mmax\n",
    "        self._level_mult = 1 / log2(M)\n",
    "        self._graphs = []\n",
    "        self._enter_point = None\n",
    "        self.data = []\n",
    "        self.visited_count = 0\n",
    "\n",
    "        ##########\n",
    "        self.visited_per_hop = []\n",
    "        self.ann_per_hop = []\n",
    "        #########\n",
    "\n",
    "    ### Algorithm 1: INSERT\n",
    "    def insert(self, q, efConstruction=None):\n",
    "\n",
    "        if efConstruction is None:\n",
    "            efConstruction = self._efConstruction\n",
    "\n",
    "        distance = self.distance_func\n",
    "        data = self.data\n",
    "        graphs = self._graphs\n",
    "        ep = self._enter_point\n",
    "        M = self._M\n",
    "\n",
    "        # line 4: determine level for the new element q\n",
    "        l = int(-log2(random()) * self._level_mult) + 1\n",
    "        idx = len(data)\n",
    "        data.append(q)\n",
    "\n",
    "        if ep is not None:\n",
    "            neg_dist = -distance(q, data[ep])\n",
    "            # distance(q, data[ep])|\n",
    "\n",
    "            # line 5-7: find the closest neighbor for levels above the insertion level\n",
    "            for lc in reversed(graphs[l:]):\n",
    "                neg_dist, ep = self._search_layer(q, [(neg_dist, ep)], lc, 1)[0]\n",
    "\n",
    "            # line 8-17: insert q at the relevant levels; W is a candidate list\n",
    "            layer0 = graphs[0]\n",
    "            W = [(neg_dist, ep)]  ## 추가\n",
    "\n",
    "            for lc in reversed(graphs[:l]):\n",
    "                M_layer = M if lc is not layer0 else self._Mmax\n",
    "\n",
    "                # line 9: update W with the closest nodes found in the graph\n",
    "                W = self._search_layer(q, W, lc, efConstruction)  ## 변경\n",
    "\n",
    "                # line 10: insert the best neighbors for q at this layer\n",
    "                lc[idx] = layer_idx = {}\n",
    "                self._select(layer_idx, W, M_layer, lc, heap=True)\n",
    "\n",
    "                # line 11-13: insert bidirectional links to the new node\n",
    "                for j, dist in layer_idx.items():\n",
    "                    self._select(lc[j], (idx, dist), M_layer, lc)\n",
    "\n",
    "        # line 18: create empty graphs for all new levels\n",
    "        for _ in range(len(graphs), l):\n",
    "            graphs.append({idx: {}})\n",
    "            self._enter_point = idx\n",
    "\n",
    "    ### Algorithm 5: K-NN-SEARCH\n",
    "    def search(self, q, K=5, efSearch=20):\n",
    "        \"\"\"Find the K points closest to q.\"\"\"\n",
    "\n",
    "        distance = self.distance_func\n",
    "        graphs = self._graphs\n",
    "        ep = self._enter_point\n",
    "        self.visited_count = 0\n",
    "\n",
    "        if ep is None:\n",
    "            raise ValueError(\"Empty graph\")\n",
    "\n",
    "        neg_dist = -distance(q, self.data[ep])\n",
    "\n",
    "        # line 1-5: search from top layers down to the second level\n",
    "        for lc in reversed(graphs[1:]):\n",
    "            neg_dist, ep = self._search_layer(q, [(neg_dist, ep)], lc, 1)[0]\n",
    "\n",
    "        ##########\n",
    "        self.visited_per_hop = []\n",
    "        self.ann_per_hop = []\n",
    "        ##########\n",
    "\n",
    "        # line 6: search with efSearch neighbors at the bottom level\n",
    "        W = self._search_layer(q, [(neg_dist, ep)], graphs[0], efSearch)\n",
    "\n",
    "        if K is not None:\n",
    "            W = nlargest(K, W)\n",
    "        else:\n",
    "            W.sort(reverse=True)\n",
    "\n",
    "        return [(idx, -md) for md, idx in W]\n",
    "\n",
    "    ### Algorithm 2: SEARCH-LAYER\n",
    "    def _search_layer(self, q, W, lc, ef):\n",
    "\n",
    "        vectorized_distance = self.vectorized_distance\n",
    "        data = self.data\n",
    "\n",
    "        # Step 1: Initialize candidate list and visited set\n",
    "        C = [(-neg_dist, idx) for neg_dist, idx in W]\n",
    "        heapify(C)\n",
    "        heapify(W)\n",
    "        visited = set(idx for _, idx in W)\n",
    "\n",
    "        # Step 4-17: Explore neighbors until candidate list is exhausted\n",
    "        while C:\n",
    "            dist, c = heappop(C)\n",
    "            furthest = -W[0][0]\n",
    "            if dist > furthest:\n",
    "                break\n",
    "            neighbors = [e for e in lc[c] if e not in visited]\n",
    "            visited.update(neighbors)\n",
    "            dists = vectorized_distance(q, [data[e] for e in neighbors])\n",
    "            for e, dist in zip(neighbors, dists):\n",
    "                self.visited_count += 1\n",
    "                neg_dist = -dist\n",
    "                if len(W) < ef:\n",
    "                    heappush(C, (dist, e))\n",
    "                    heappush(W, (neg_dist, e))\n",
    "                    furthest = -W[0][0]\n",
    "                elif dist < furthest:\n",
    "                    heappush(C, (dist, e))\n",
    "                    heapreplace(W, (neg_dist, e))\n",
    "                    furthest = -W[0][0]\n",
    "\n",
    "            ##########\n",
    "            self.visited_per_hop.append(len(visited))\n",
    "            topk = nsmallest(min(ef, len(W)), ((-neg, idx) for neg, idx in W))  # (dist, id)\n",
    "            self.ann_per_hop.append([idx for _, idx in topk])\n",
    "            ##########\n",
    "\n",
    "        return W\n",
    "\n",
    "    ### Algorithm 3: SELECT-NEIGHBORS-SIMPLE\n",
    "    def _select(self, R, C, M, lc, heap=False):\n",
    "\n",
    "        if not heap:\n",
    "            idx, dist = C\n",
    "            if len(R) < M:\n",
    "                R[idx] = dist\n",
    "            else:\n",
    "                max_idx, max_dist = max(R.items(), key=itemgetter(1))\n",
    "                if dist < max_dist:\n",
    "                    del R[max_idx]\n",
    "                    R[idx] = dist\n",
    "            return\n",
    "\n",
    "        else:\n",
    "            C = nlargest(M, C)\n",
    "            R.update({idx: -neg_dist for neg_dist, idx in C})\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ly4EhHnP3AC7"
   },
   "source": [
    "# Data preparation"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## SIFT1M Dataset"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T06:37:47.196200Z",
     "start_time": "2025-10-14T06:37:46.733817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Load SIFT1M dataset (.fvecs / .ivecs)\n",
    "import struct\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def read_fvecs(filename):\n",
    "    \"\"\"Reads .fvecs binary file into np.ndarray of shape (n, d).\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = f.read()\n",
    "    dim = struct.unpack('i', data[:4])[0]\n",
    "    vecs = np.frombuffer(data, dtype=np.float32)\n",
    "    vecs = vecs.reshape(-1, dim + 1)[:, 1:]  # drop the leading 'dim'\n",
    "    return vecs\n",
    "\n",
    "def read_ivecs(filename):\n",
    "    \"\"\"Reads .ivecs binary file into np.ndarray of shape (n, k).\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = f.read()\n",
    "    dim = struct.unpack('i', data[:4])[0]\n",
    "    vecs = np.frombuffer(data, dtype=np.int32)\n",
    "    vecs = vecs.reshape(-1, dim + 1)[:, 1:]\n",
    "    return vecs\n",
    "\n",
    "# 데이터셋 경로 (현재 구조에 맞춰 수정)\n",
    "base_path = \"../datasets\"\n",
    "train = read_fvecs(f\"{base_path}/sift_base.fvecs\")  # 1,000,000 × 128\n",
    "test = read_fvecs(f\"{base_path}/sift_query.fvecs\")  # 10,000 × 128\n",
    "neighbors = read_ivecs(f\"{base_path}/sift_groundtruth.ivecs\")  # 10,000 × 100\n",
    "\n",
    "# Original dataset (1,000,000 × 128)\n",
    "original_data = train  # Assuming `train` contains the original dataset\n",
    "# Sampling size\n",
    "sample_size = 300_000\n",
    "# Random sampling (set random seed for reproducibility)\n",
    "np.random.seed(42)\n",
    "sampled_indices = np.random.choice(len(original_data), size=sample_size, replace=False)\n",
    "train = original_data[sampled_indices]\n",
    "# find maximum value and minimum\n",
    "\n",
    "\n",
    "print(\"train:\", train.shape, \"test:\", test.shape, \"neighbors:\", neighbors.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 36.  21.   0.   0.   0.   0.   0.  15.  47. 115.  13.   7.   0.   0.\n",
      "   0.   8.   5.  20.  11.  60.  56.   4.   0.   0.   0.   0.   0.  44.\n",
      "  99.   0.   0.   0.  55. 118.   2.   0.   0.   0.   7.  31.  51. 118.\n",
      "  10.  10.   7.   5.   3.  25.   2.   6.   7.  96.  87.  32.   2.   0.\n",
      "   0.   1.  19.  62. 118.   8.   0.   0.  53.  14.   0.   2.   7.   9.\n",
      "  68. 111.  80.   5.   1.   1.   3.  76.  80. 118.  26.   5.   1.   5.\n",
      "  24. 118. 114.  28.  35.  41.  68.  28.  30.  19.   9.  11.  23.   9.\n",
      "  27.   8.  14.   7.  11.  24.  22.   4.  49.  22.   6.  78.  92.  46.\n",
      "  97.  40.  36.   3.   0.  62.  58.  37. 118.  29.   6.  10.   9.   0.\n",
      "   0.  14.]\n",
      "train: (300000, 128) test: (10000, 128) neighbors: (10000, 100)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T04:03:31.208806Z",
     "start_time": "2025-10-14T04:02:58.110468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def exact_topk_l2(train_subset, queries, K):\n",
    "    out = np.empty((len(queries), K), dtype=np.int32)\n",
    "    for i, q in enumerate(queries):\n",
    "        d = np.sum((train_subset - q)**2, axis=1)      # L2^2\n",
    "        idx = np.argpartition(d, K)[:K]                # top-K (unordered)\n",
    "        idx = idx[np.argsort(d[idx])]                  # sort by distance\n",
    "        out[i] = idx\n",
    "    return out\n",
    "\n",
    "K_value = 10\n",
    "# 예: 쿼리 1000개만 먼저\n",
    "neighbors_subset = exact_topk_l2(train, test[:1000], max(100, K_value))"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## OpenAI Embedding Dataset (For Prototype)"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SNKeUoSW2_QW",
    "outputId": "e241eaa8-ce67-47c9-8cc1-7cd32207f854",
    "ExecuteTime": {
     "end_time": "2025-10-14T04:03:31.217261Z",
     "start_time": "2025-10-14T04:03:31.215180Z"
    }
   },
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import h5py\n",
    "# import time\n",
    "#\n",
    "# f = h5py.File('./openai_embedding_output.hdf5', 'r')\n",
    "# distances = f['distances']\n",
    "# neighbors = f['neighbors']\n",
    "# test = f['test']\n",
    "# train = f['train']"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9o1p9JoD9Wcg"
   },
   "source": [
    "# Comparing (Naive Insert VS Clustered Insert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUb4bTYqARa0"
   },
   "source": [
    "## Naive Insert"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ObbmSAjZAgu6",
    "ExecuteTime": {
     "end_time": "2025-10-14T04:28:16.625503Z",
     "start_time": "2025-10-14T04:03:31.225811Z"
    }
   },
   "source": [
    "naivehnsw = HNSW(\"l2\", M=16, efConstruction=64)\n",
    "for i in range(len(train)):\n",
    "    if i % 1000 == 0:\n",
    "        print(\"Inserting data point:\", i)\n",
    "    naivehnsw.insert(train[i])\n",
    "\n",
    "with open('naive_hnsw_model.pkl', 'wb') as f:\n",
    "    pickle.dump(naivehnsw, f)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting data point: 0\n",
      "Inserting data point: 1000\n",
      "Inserting data point: 2000\n",
      "Inserting data point: 3000\n",
      "Inserting data point: 4000\n",
      "Inserting data point: 5000\n",
      "Inserting data point: 6000\n",
      "Inserting data point: 7000\n",
      "Inserting data point: 8000\n",
      "Inserting data point: 9000\n",
      "Inserting data point: 10000\n",
      "Inserting data point: 11000\n",
      "Inserting data point: 12000\n",
      "Inserting data point: 13000\n",
      "Inserting data point: 14000\n",
      "Inserting data point: 15000\n",
      "Inserting data point: 16000\n",
      "Inserting data point: 17000\n",
      "Inserting data point: 18000\n",
      "Inserting data point: 19000\n",
      "Inserting data point: 20000\n",
      "Inserting data point: 21000\n",
      "Inserting data point: 22000\n",
      "Inserting data point: 23000\n",
      "Inserting data point: 24000\n",
      "Inserting data point: 25000\n",
      "Inserting data point: 26000\n",
      "Inserting data point: 27000\n",
      "Inserting data point: 28000\n",
      "Inserting data point: 29000\n",
      "Inserting data point: 30000\n",
      "Inserting data point: 31000\n",
      "Inserting data point: 32000\n",
      "Inserting data point: 33000\n",
      "Inserting data point: 34000\n",
      "Inserting data point: 35000\n",
      "Inserting data point: 36000\n",
      "Inserting data point: 37000\n",
      "Inserting data point: 38000\n",
      "Inserting data point: 39000\n",
      "Inserting data point: 40000\n",
      "Inserting data point: 41000\n",
      "Inserting data point: 42000\n",
      "Inserting data point: 43000\n",
      "Inserting data point: 44000\n",
      "Inserting data point: 45000\n",
      "Inserting data point: 46000\n",
      "Inserting data point: 47000\n",
      "Inserting data point: 48000\n",
      "Inserting data point: 49000\n",
      "Inserting data point: 50000\n",
      "Inserting data point: 51000\n",
      "Inserting data point: 52000\n",
      "Inserting data point: 53000\n",
      "Inserting data point: 54000\n",
      "Inserting data point: 55000\n",
      "Inserting data point: 56000\n",
      "Inserting data point: 57000\n",
      "Inserting data point: 58000\n",
      "Inserting data point: 59000\n",
      "Inserting data point: 60000\n",
      "Inserting data point: 61000\n",
      "Inserting data point: 62000\n",
      "Inserting data point: 63000\n",
      "Inserting data point: 64000\n",
      "Inserting data point: 65000\n",
      "Inserting data point: 66000\n",
      "Inserting data point: 67000\n",
      "Inserting data point: 68000\n",
      "Inserting data point: 69000\n",
      "Inserting data point: 70000\n",
      "Inserting data point: 71000\n",
      "Inserting data point: 72000\n",
      "Inserting data point: 73000\n",
      "Inserting data point: 74000\n",
      "Inserting data point: 75000\n",
      "Inserting data point: 76000\n",
      "Inserting data point: 77000\n",
      "Inserting data point: 78000\n",
      "Inserting data point: 79000\n",
      "Inserting data point: 80000\n",
      "Inserting data point: 81000\n",
      "Inserting data point: 82000\n",
      "Inserting data point: 83000\n",
      "Inserting data point: 84000\n",
      "Inserting data point: 85000\n",
      "Inserting data point: 86000\n",
      "Inserting data point: 87000\n",
      "Inserting data point: 88000\n",
      "Inserting data point: 89000\n",
      "Inserting data point: 90000\n",
      "Inserting data point: 91000\n",
      "Inserting data point: 92000\n",
      "Inserting data point: 93000\n",
      "Inserting data point: 94000\n",
      "Inserting data point: 95000\n",
      "Inserting data point: 96000\n",
      "Inserting data point: 97000\n",
      "Inserting data point: 98000\n",
      "Inserting data point: 99000\n",
      "Inserting data point: 100000\n",
      "Inserting data point: 101000\n",
      "Inserting data point: 102000\n",
      "Inserting data point: 103000\n",
      "Inserting data point: 104000\n",
      "Inserting data point: 105000\n",
      "Inserting data point: 106000\n",
      "Inserting data point: 107000\n",
      "Inserting data point: 108000\n",
      "Inserting data point: 109000\n",
      "Inserting data point: 110000\n",
      "Inserting data point: 111000\n",
      "Inserting data point: 112000\n",
      "Inserting data point: 113000\n",
      "Inserting data point: 114000\n",
      "Inserting data point: 115000\n",
      "Inserting data point: 116000\n",
      "Inserting data point: 117000\n",
      "Inserting data point: 118000\n",
      "Inserting data point: 119000\n",
      "Inserting data point: 120000\n",
      "Inserting data point: 121000\n",
      "Inserting data point: 122000\n",
      "Inserting data point: 123000\n",
      "Inserting data point: 124000\n",
      "Inserting data point: 125000\n",
      "Inserting data point: 126000\n",
      "Inserting data point: 127000\n",
      "Inserting data point: 128000\n",
      "Inserting data point: 129000\n",
      "Inserting data point: 130000\n",
      "Inserting data point: 131000\n",
      "Inserting data point: 132000\n",
      "Inserting data point: 133000\n",
      "Inserting data point: 134000\n",
      "Inserting data point: 135000\n",
      "Inserting data point: 136000\n",
      "Inserting data point: 137000\n",
      "Inserting data point: 138000\n",
      "Inserting data point: 139000\n",
      "Inserting data point: 140000\n",
      "Inserting data point: 141000\n",
      "Inserting data point: 142000\n",
      "Inserting data point: 143000\n",
      "Inserting data point: 144000\n",
      "Inserting data point: 145000\n",
      "Inserting data point: 146000\n",
      "Inserting data point: 147000\n",
      "Inserting data point: 148000\n",
      "Inserting data point: 149000\n",
      "Inserting data point: 150000\n",
      "Inserting data point: 151000\n",
      "Inserting data point: 152000\n",
      "Inserting data point: 153000\n",
      "Inserting data point: 154000\n",
      "Inserting data point: 155000\n",
      "Inserting data point: 156000\n",
      "Inserting data point: 157000\n",
      "Inserting data point: 158000\n",
      "Inserting data point: 159000\n",
      "Inserting data point: 160000\n",
      "Inserting data point: 161000\n",
      "Inserting data point: 162000\n",
      "Inserting data point: 163000\n",
      "Inserting data point: 164000\n",
      "Inserting data point: 165000\n",
      "Inserting data point: 166000\n",
      "Inserting data point: 167000\n",
      "Inserting data point: 168000\n",
      "Inserting data point: 169000\n",
      "Inserting data point: 170000\n",
      "Inserting data point: 171000\n",
      "Inserting data point: 172000\n",
      "Inserting data point: 173000\n",
      "Inserting data point: 174000\n",
      "Inserting data point: 175000\n",
      "Inserting data point: 176000\n",
      "Inserting data point: 177000\n",
      "Inserting data point: 178000\n",
      "Inserting data point: 179000\n",
      "Inserting data point: 180000\n",
      "Inserting data point: 181000\n",
      "Inserting data point: 182000\n",
      "Inserting data point: 183000\n",
      "Inserting data point: 184000\n",
      "Inserting data point: 185000\n",
      "Inserting data point: 186000\n",
      "Inserting data point: 187000\n",
      "Inserting data point: 188000\n",
      "Inserting data point: 189000\n",
      "Inserting data point: 190000\n",
      "Inserting data point: 191000\n",
      "Inserting data point: 192000\n",
      "Inserting data point: 193000\n",
      "Inserting data point: 194000\n",
      "Inserting data point: 195000\n",
      "Inserting data point: 196000\n",
      "Inserting data point: 197000\n",
      "Inserting data point: 198000\n",
      "Inserting data point: 199000\n",
      "Inserting data point: 200000\n",
      "Inserting data point: 201000\n",
      "Inserting data point: 202000\n",
      "Inserting data point: 203000\n",
      "Inserting data point: 204000\n",
      "Inserting data point: 205000\n",
      "Inserting data point: 206000\n",
      "Inserting data point: 207000\n",
      "Inserting data point: 208000\n",
      "Inserting data point: 209000\n",
      "Inserting data point: 210000\n",
      "Inserting data point: 211000\n",
      "Inserting data point: 212000\n",
      "Inserting data point: 213000\n",
      "Inserting data point: 214000\n",
      "Inserting data point: 215000\n",
      "Inserting data point: 216000\n",
      "Inserting data point: 217000\n",
      "Inserting data point: 218000\n",
      "Inserting data point: 219000\n",
      "Inserting data point: 220000\n",
      "Inserting data point: 221000\n",
      "Inserting data point: 222000\n",
      "Inserting data point: 223000\n",
      "Inserting data point: 224000\n",
      "Inserting data point: 225000\n",
      "Inserting data point: 226000\n",
      "Inserting data point: 227000\n",
      "Inserting data point: 228000\n",
      "Inserting data point: 229000\n",
      "Inserting data point: 230000\n",
      "Inserting data point: 231000\n",
      "Inserting data point: 232000\n",
      "Inserting data point: 233000\n",
      "Inserting data point: 234000\n",
      "Inserting data point: 235000\n",
      "Inserting data point: 236000\n",
      "Inserting data point: 237000\n",
      "Inserting data point: 238000\n",
      "Inserting data point: 239000\n",
      "Inserting data point: 240000\n",
      "Inserting data point: 241000\n",
      "Inserting data point: 242000\n",
      "Inserting data point: 243000\n",
      "Inserting data point: 244000\n",
      "Inserting data point: 245000\n",
      "Inserting data point: 246000\n",
      "Inserting data point: 247000\n",
      "Inserting data point: 248000\n",
      "Inserting data point: 249000\n",
      "Inserting data point: 250000\n",
      "Inserting data point: 251000\n",
      "Inserting data point: 252000\n",
      "Inserting data point: 253000\n",
      "Inserting data point: 254000\n",
      "Inserting data point: 255000\n",
      "Inserting data point: 256000\n",
      "Inserting data point: 257000\n",
      "Inserting data point: 258000\n",
      "Inserting data point: 259000\n",
      "Inserting data point: 260000\n",
      "Inserting data point: 261000\n",
      "Inserting data point: 262000\n",
      "Inserting data point: 263000\n",
      "Inserting data point: 264000\n",
      "Inserting data point: 265000\n",
      "Inserting data point: 266000\n",
      "Inserting data point: 267000\n",
      "Inserting data point: 268000\n",
      "Inserting data point: 269000\n",
      "Inserting data point: 270000\n",
      "Inserting data point: 271000\n",
      "Inserting data point: 272000\n",
      "Inserting data point: 273000\n",
      "Inserting data point: 274000\n",
      "Inserting data point: 275000\n",
      "Inserting data point: 276000\n",
      "Inserting data point: 277000\n",
      "Inserting data point: 278000\n",
      "Inserting data point: 279000\n",
      "Inserting data point: 280000\n",
      "Inserting data point: 281000\n",
      "Inserting data point: 282000\n",
      "Inserting data point: 283000\n",
      "Inserting data point: 284000\n",
      "Inserting data point: 285000\n",
      "Inserting data point: 286000\n",
      "Inserting data point: 287000\n",
      "Inserting data point: 288000\n",
      "Inserting data point: 289000\n",
      "Inserting data point: 290000\n",
      "Inserting data point: 291000\n",
      "Inserting data point: 292000\n",
      "Inserting data point: 293000\n",
      "Inserting data point: 294000\n",
      "Inserting data point: 295000\n",
      "Inserting data point: 296000\n",
      "Inserting data point: 297000\n",
      "Inserting data point: 298000\n",
      "Inserting data point: 299000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m     naivehnsw\u001B[38;5;241m.\u001B[39minsert(train[i])\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnaive_hnsw_model.pkl\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m----> 8\u001B[0m     \u001B[43mpickle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdump\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnaivehnsw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqK_swd5AZRb"
   },
   "source": [
    "## Clustered Insert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5eM0AB33_NR"
   },
   "source": [
    "### Data clustering (with K-means)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T04:28:16.856350Z",
     "start_time": "2025-10-14T00:34:38.686131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=100, n_init='auto', random_state=21).fit(train)\n",
    "labels = kmeans.labels_  # 각 벡터가 속한 클러스터 번호\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "cluster_data = defaultdict(list)\n",
    "for i, label in enumerate(labels):\n",
    "    cluster_data[label].append((i, train[i]))  # Store a tuple of (original_index, data_point)\n",
    "\n",
    "# # numpy 배열로 변환 - No longer needed as we need to preserve original indices\n",
    "# cluster_data = {k: np.array(v) for k, v in cluster_data.items()}"
   ],
   "outputs": [],
   "execution_count": 65
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVUd35Xw376-"
   },
   "source": "### Cluster-wise Data insertion #1 (by cluster)"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "S_b1MkFx3lIs",
    "ExecuteTime": {
     "end_time": "2025-10-14T04:28:17.286985Z",
     "start_time": "2025-10-14T00:34:49.035328Z"
    }
   },
   "source": [
    "hnswWithClusteredInput = HNSW(\"l2\", M=16, efConstruction=64)\n",
    "cluster_insertion_order = []\n",
    "for i in range(len(cluster_data)):\n",
    "    print(\"Inserting Cluster Number: \", i)\n",
    "    # Iterate through the list of (original_index, data_point) tuples in each cluster\n",
    "    for original_index, data_point in cluster_data[i]:\n",
    "        cluster_insertion_order.append(int(original_index))  # Append the original index to the insertion order\n",
    "        hnswWithClusteredInput.insert(data_point)\n",
    "\n",
    "with open('clustered_hnsw_model.pkl', 'wb') as f:\n",
    "    pickle.dump(hnswWithClusteredInput, f)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting Cluster Number:  0\n",
      "Inserting Cluster Number:  1\n",
      "Inserting Cluster Number:  2\n",
      "Inserting Cluster Number:  3\n",
      "Inserting Cluster Number:  4\n",
      "Inserting Cluster Number:  5\n",
      "Inserting Cluster Number:  6\n",
      "Inserting Cluster Number:  7\n",
      "Inserting Cluster Number:  8\n",
      "Inserting Cluster Number:  9\n",
      "Inserting Cluster Number:  10\n",
      "Inserting Cluster Number:  11\n",
      "Inserting Cluster Number:  12\n",
      "Inserting Cluster Number:  13\n",
      "Inserting Cluster Number:  14\n",
      "Inserting Cluster Number:  15\n",
      "Inserting Cluster Number:  16\n",
      "Inserting Cluster Number:  17\n",
      "Inserting Cluster Number:  18\n",
      "Inserting Cluster Number:  19\n",
      "Inserting Cluster Number:  20\n",
      "Inserting Cluster Number:  21\n",
      "Inserting Cluster Number:  22\n",
      "Inserting Cluster Number:  23\n",
      "Inserting Cluster Number:  24\n",
      "Inserting Cluster Number:  25\n",
      "Inserting Cluster Number:  26\n",
      "Inserting Cluster Number:  27\n",
      "Inserting Cluster Number:  28\n",
      "Inserting Cluster Number:  29\n",
      "Inserting Cluster Number:  30\n",
      "Inserting Cluster Number:  31\n",
      "Inserting Cluster Number:  32\n",
      "Inserting Cluster Number:  33\n",
      "Inserting Cluster Number:  34\n",
      "Inserting Cluster Number:  35\n",
      "Inserting Cluster Number:  36\n",
      "Inserting Cluster Number:  37\n",
      "Inserting Cluster Number:  38\n",
      "Inserting Cluster Number:  39\n",
      "Inserting Cluster Number:  40\n",
      "Inserting Cluster Number:  41\n",
      "Inserting Cluster Number:  42\n",
      "Inserting Cluster Number:  43\n",
      "Inserting Cluster Number:  44\n",
      "Inserting Cluster Number:  45\n",
      "Inserting Cluster Number:  46\n",
      "Inserting Cluster Number:  47\n",
      "Inserting Cluster Number:  48\n",
      "Inserting Cluster Number:  49\n",
      "Inserting Cluster Number:  50\n",
      "Inserting Cluster Number:  51\n",
      "Inserting Cluster Number:  52\n",
      "Inserting Cluster Number:  53\n",
      "Inserting Cluster Number:  54\n",
      "Inserting Cluster Number:  55\n",
      "Inserting Cluster Number:  56\n",
      "Inserting Cluster Number:  57\n",
      "Inserting Cluster Number:  58\n",
      "Inserting Cluster Number:  59\n",
      "Inserting Cluster Number:  60\n",
      "Inserting Cluster Number:  61\n",
      "Inserting Cluster Number:  62\n",
      "Inserting Cluster Number:  63\n",
      "Inserting Cluster Number:  64\n",
      "Inserting Cluster Number:  65\n",
      "Inserting Cluster Number:  66\n",
      "Inserting Cluster Number:  67\n",
      "Inserting Cluster Number:  68\n",
      "Inserting Cluster Number:  69\n",
      "Inserting Cluster Number:  70\n",
      "Inserting Cluster Number:  71\n",
      "Inserting Cluster Number:  72\n",
      "Inserting Cluster Number:  73\n",
      "Inserting Cluster Number:  74\n",
      "Inserting Cluster Number:  75\n",
      "Inserting Cluster Number:  76\n",
      "Inserting Cluster Number:  77\n",
      "Inserting Cluster Number:  78\n",
      "Inserting Cluster Number:  79\n",
      "Inserting Cluster Number:  80\n",
      "Inserting Cluster Number:  81\n",
      "Inserting Cluster Number:  82\n",
      "Inserting Cluster Number:  83\n",
      "Inserting Cluster Number:  84\n",
      "Inserting Cluster Number:  85\n",
      "Inserting Cluster Number:  86\n",
      "Inserting Cluster Number:  87\n",
      "Inserting Cluster Number:  88\n",
      "Inserting Cluster Number:  89\n",
      "Inserting Cluster Number:  90\n",
      "Inserting Cluster Number:  91\n",
      "Inserting Cluster Number:  92\n",
      "Inserting Cluster Number:  93\n",
      "Inserting Cluster Number:  94\n",
      "Inserting Cluster Number:  95\n",
      "Inserting Cluster Number:  96\n",
      "Inserting Cluster Number:  97\n",
      "Inserting Cluster Number:  98\n",
      "Inserting Cluster Number:  99\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cluster round-robin insertion #2 (by cluster in round-robin order)"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T04:28:17.416402Z",
     "start_time": "2025-10-14T01:12:22.811349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hnswWithClusterRRInput = HNSW(\"l2\", M=16, efConstruction=64)\n",
    "cluster_rr_insertion_order = []\n",
    "max_cluster_size = max(len(v) for v in cluster_data.values())\n",
    "for j in range(max_cluster_size):\n",
    "    for i in range(len(cluster_data)):\n",
    "        if j < len(cluster_data[i]):\n",
    "            original_index, data_point = cluster_data[i][j]\n",
    "            cluster_rr_insertion_order.append(int(original_index))  # Append the original index to the insertion order\n",
    "            hnswWithClusterRRInput.insert(data_point)\n",
    "\n",
    "with open('clusteredRR_hnsw_model.pkl', 'wb') as f:\n",
    "    pickle.dump(hnswWithClusterRRInput, f)"
   ],
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Clustered Data insertion #3 (Two-phase cluster insertion)"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2025-10-14T04:28:17.447579Z",
     "start_time": "2025-10-14T01:59:39.487040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Two-phase clustered insertion:\n",
    "#  (Phase 1) seed insertion: insert a small fraction from each cluster in round-robin order\n",
    "#  (Phase 2) bulk insertion: insert all remaining items cluster-by-cluster\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameters for the two-phase strategy\n",
    "seed_ratio = 0.2          # insert first 5% per cluster during Phase 1 (set to 0 to disable)\n",
    "min_seed_per_cluster = 5   # ensure at least a few seeds per cluster\n",
    "rng = np.random.default_rng(42)  # reproducible sampling inside each cluster\n",
    "\n",
    "# Initialize index for the mixed strategy\n",
    "hnswWithMixedClusteredInput = HNSW(\"l2\", M=16, efConstruction=64)\n",
    "mixed_cluster_insertion_order = []  # k-th inserted -> original train index (for id-space mapping)\n",
    "\n",
    "# Build per-cluster arrays (copy from cluster_data to ensure deterministic order/shuffle per cluster)\n",
    "clusters = sorted(cluster_data.keys())\n",
    "per_cluster_items = {c: list(cluster_data[c]) for c in clusters}  # list of (orig_idx, vec)\n",
    "\n",
    "# Shuffle within each cluster (optional but recommended to avoid within-cluster sorting bias)\n",
    "for c in clusters:\n",
    "    if len(per_cluster_items[c]) > 1:\n",
    "        rng.shuffle(per_cluster_items[c])\n",
    "\n",
    "# Determine seed sets and remaining sets per cluster\n",
    "seed_sets = {}\n",
    "remain_sets = {}\n",
    "for c in clusters:\n",
    "    n = len(per_cluster_items[c])\n",
    "    seed_n = min(n, max(min_seed_per_cluster, int(n * seed_ratio)))\n",
    "    seed_sets[c] = per_cluster_items[c][:seed_n]\n",
    "    remain_sets[c] = per_cluster_items[c][seed_n:]\n",
    "\n",
    "# -----------------\n",
    "# Phase 1: round-robin insertion across clusters (seed points)\n",
    "# -----------------\n",
    "# Create iterators for each seed list\n",
    "seed_iters = {c: iter(seed_sets[c]) for c in clusters}\n",
    "inserted_seed_counts = defaultdict(int)\n",
    "print(\"[MixedClustered] Phase 1: inserting seeds per cluster:\", {c: len(seed_sets[c]) for c in clusters})\n",
    "# Round-robin until all seed iters are exhausted\n",
    "active = set(clusters)\n",
    "while active:\n",
    "    for c in list(active):\n",
    "        try:\n",
    "            orig_idx, vec = next(seed_iters[c])\n",
    "            mixed_cluster_insertion_order.append(int(orig_idx))\n",
    "            hnswWithMixedClusteredInput.insert(vec)\n",
    "            inserted_seed_counts[c] += 1\n",
    "        except StopIteration:\n",
    "            active.remove(c)\n",
    "\n",
    "# -----------------\n",
    "# Phase 2: bulk insertion cluster-by-cluster (remaining points)\n",
    "# -----------------\n",
    "print(\"[MixedClustered] Phase 2: inserting remaining items per cluster:\", {c: len(remain_sets[c]) for c in clusters})\n",
    "for c in clusters:\n",
    "    print(\"Inserting remaining items of Cluster Number: \", c)\n",
    "    # Insert the remaining items of cluster c\n",
    "    for orig_idx, vec in remain_sets[c]:\n",
    "        mixed_cluster_insertion_order.append(int(orig_idx))\n",
    "        hnswWithMixedClusteredInput.insert(vec)\n",
    "\n",
    "print(\"[MixedClustered] Phase1(seeds) inserted per cluster:\", {c: inserted_seed_counts[c] for c in clusters})\n",
    "print(\"[MixedClustered] Total inserted:\", len(mixed_cluster_insertion_order))\n",
    "\n",
    "with open('mixedClustered_hnsw_model.pkl', 'wb') as f:\n",
    "    pickle.dump(hnswWithMixedClusteredInput, f)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zul7ASBvJrm3"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T04:28:17.448558Z",
     "start_time": "2025-10-13T10:07:22.044400Z"
    }
   },
   "source": [
    "def calculate_recall(true_neighbors, predicted_neighbors, K):\n",
    "    K = int(K)\n",
    "    \"\"\"Calculates recall for a single query.\"\"\"\n",
    "    true_topk = set(map(int, true_neighbors[:K]))\n",
    "    pred_topk = set(map(int, predicted_neighbors[:K]))\n",
    "    return len(true_topk & pred_topk) / K"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# data structure with key(string) and value for list of visited nodes per query\n",
    "visited_nodes_per_query = {}\n",
    "recall_per_query = {}"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# let's load model from files\n",
    "import pickle\n",
    "with open('./models/efConstruction64_M16/naive_hnsw_model.pkl', 'rb') as f:\n",
    "    naivehnsw = pickle.load(f)\n",
    "with open('./models/efConstruction64_M16/clustered_hnsw_model.pkl', 'rb') as f:\n",
    "    hnswWithClusteredInput = pickle.load(f)\n",
    "with open('./models/efConstruction64_M16/clusteredRR_hnsw_model.pkl', 'rb') as f:\n",
    "    hnswWithClusterRRInput = pickle.load(f)\n",
    "with open('./models/efConstruction64_M16/mixedClustered_hnsw_model.pkl', 'rb') as f:\n",
    "    hnswWithMixedClusteredInput = pickle.load(f)\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T04:28:17.475209Z",
     "start_time": "2025-10-13T14:05:33.991730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "def last_or_zero(seq):\n",
    "    return int(seq[-1]) if isinstance(seq, (list, tuple)) and len(seq) > 0 else 0\n",
    "\n",
    "efSearch = [20, 40, 100, 400, 1000]\n",
    "\n",
    "for ef in efSearch:\n",
    "\n",
    "    naive_visited_nodes = []\n",
    "    naive_recall = []\n",
    "\n",
    "    print(\"Evaluating efSearch =\", ef)\n",
    "    print(\"Naive Insertion Search with efSearch =\", ef)\n",
    "    K_value = 10  # Assuming K=10 for recall calculation as per search results\n",
    "    for i in range(1000):\n",
    "        query = test[i]\n",
    "        query_true_neighbors = neighbors_subset[i]\n",
    "        naive_search_results = naivehnsw.search(query, K=K_value, efSearch=ef)\n",
    "        naive_visited_nodes.append(list(naivehnsw.visited_per_hop))  # Store a copy of visited_per_hop\n",
    "\n",
    "        naive_search_results_indices = [idx for idx, dist in naive_search_results][:K_value]\n",
    "        naive_recall.append(calculate_recall(query_true_neighbors, naive_search_results_indices, K_value))\n",
    "\n",
    "    clustered_visited_nodes = []\n",
    "    clustered_recall = []\n",
    "    print(\"Clustered Insertion Search with efSearch =\", ef)\n",
    "\n",
    "    K_value = 10  # Assuming K=10 for recall calculation as per search results\n",
    "    for i in range(1000):\n",
    "        query = test[i]\n",
    "        query_true_neighbors = neighbors_subset[i]\n",
    "        clustered_search_results = hnswWithClusteredInput.search(query, K=K_value, efSearch=ef)\n",
    "        clustered_visited_nodes.append(list(hnswWithClusteredInput.visited_per_hop))  # Store a copy of visited_per_hop\n",
    "\n",
    "        clustered_search_indices = [idx for idx, dist in clustered_search_results]\n",
    "        clustered_search_orig_ids = [cluster_insertion_order[idx] for idx in clustered_search_indices][:K_value]\n",
    "        clustered_recall.append(calculate_recall(query_true_neighbors, clustered_search_orig_ids, K_value))\n",
    "\n",
    "\n",
    "    print(\"Clustered Round-Robin Insertion Search with efSearch =\", ef)\n",
    "    clustered_RR_visited_nodes = []\n",
    "    clustered_RR_recall = []\n",
    "\n",
    "    K_value = 10  # Assuming K=10 for recall calculation as per search results\n",
    "    for i in range(1000):\n",
    "        query = test[i]\n",
    "        query_true_neighbors = neighbors_subset[i]\n",
    "        clustered_RR_search_results = hnswWithClusteredInput.search(query, K=K_value, efSearch=ef)\n",
    "        clustered_RR_visited_nodes.append(list(hnswWithClusterRRInput.visited_per_hop))  # Store a copy of visited_per_hop\n",
    "\n",
    "        clustered_RR_search_indices = [idx for idx, dist in clustered_RR_search_results]\n",
    "        clustered_RR_search_orig_ids = [cluster_insertion_order[idx] for idx in clustered_RR_search_indices][:K_value]\n",
    "        clustered_RR_recall.append(calculate_recall(query_true_neighbors, clustered_RR_search_orig_ids, K_value))\n",
    "\n",
    "    mixed_clustered_visited_nodes = []\n",
    "    mixed_clustered_recall = []\n",
    "\n",
    "    print(\"Mixed Clustered Insertion Search with efSearch =\", ef)\n",
    "    K_value = 10  # Assuming K=10 for recall calculation as per search results\n",
    "    for i in range(1000):\n",
    "        query = test[i]\n",
    "        query_true_neighbors = neighbors_subset[i]\n",
    "        mixed_clustered_search_results = hnswWithMixedClusteredInput.search(query, K=K_value, efSearch=ef)\n",
    "        mixed_clustered_visited_nodes.append(list(hnswWithMixedClusteredInput.visited_per_hop))  # Store a copy of visited_per_hop\n",
    "\n",
    "        mixed_clustered_search_indices = [idx for idx, dist in mixed_clustered_search_results]\n",
    "        mixed_clustered_search_orig_ids = [mixed_cluster_insertion_order[idx] for idx in mixed_clustered_search_indices][:K_value]\n",
    "        mixed_clustered_recall.append(calculate_recall(query_true_neighbors, mixed_clustered_search_orig_ids, K_value))\n",
    "\n",
    "\n",
    "    # total unique visited nodes per query\n",
    "    naive_total = [last_or_zero(v) for v in naive_visited_nodes]\n",
    "    clustered_total = [last_or_zero(v) for v in clustered_visited_nodes]\n",
    "    clustered_rr_total = [last_or_zero(v) for v in clustered_RR_visited_nodes]\n",
    "    mixed_total = [last_or_zero(v) for v in mixed_clustered_visited_nodes] if 'mixed_clustered_visited_nodes' in globals() else None\n",
    "\n",
    "    # store visited_node results for this ef\n",
    "    visited_nodes_per_query[f\"naive_ef{ef}\"] = np.asarray(naive_total, dtype=np.int64).mean()\n",
    "    visited_nodes_per_query[f\"clustered_ef{ef}\"] = np.asarray(clustered_total, dtype=np.int64).mean()\n",
    "    visited_nodes_per_query[f\"clusteredRR_ef{ef}\"] = np.asarray(clustered_rr_total, dtype=np.int64).mean()\n",
    "    if mixed_total is not None:\n",
    "        visited_nodes_per_query[f\"mixed_ef{ef}\"] = np.asarray(mixed_total, dtype=np.int64).mean()\n",
    "\n",
    "    print(\"Average visited nodes - Naive:\", visited_nodes_per_query[f\"naive_ef{ef}\"])\n",
    "    print(\"Average visited nodes - Clustered:\", visited_nodes_per_query[f\"clustered_ef{ef}\"])\n",
    "    print(\"Average visited nodes - ClusteredRR:\", visited_nodes_per_query[f\"clusteredRR_ef{ef}\"])\n",
    "    if mixed_total is not None:\n",
    "        print(\"Average visited nodes - Mixed:\", visited_nodes_per_query[f\"mixed_ef{ef}\"])\n",
    "\n",
    "    # store recall results for this ef\n",
    "    recall_per_query[f\"naive_ef{ef}\"] = np.asarray(naive_recall, dtype=np.float32).mean()\n",
    "    recall_per_query[f\"clustered_ef{ef}\"] = np.asarray(clustered_recall, dtype=np.float32).mean()\n",
    "    recall_per_query[f\"clusteredRR_ef{ef}\"] = np.asarray(clustered_RR_recall, dtype=np.float32).mean()\n",
    "    if mixed_clustered_recall is not None:\n",
    "        recall_per_query[f\"mixed_ef{ef}\"] = np.asarray(mixed_clustered_recall, dtype=np.float32).mean()\n",
    "    print(\"Average Recall:\", recall_per_query[f\"naive_ef{ef}\"])\n",
    "    print(\"Average Recall:\", recall_per_query[f\"clustered_ef{ef}\"])\n",
    "    print(\"Average Recall:\", recall_per_query[f\"clusteredRR_ef{ef}\"])\n",
    "    if mixed_clustered_recall is not None:\n",
    "        print(\"Average Recall:\", recall_per_query[f\"mixed_ef{ef}\"])\n",
    "\n",
    "    print(f\"efSearch={ef} done.\")\n"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Results"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Show results as a table\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame({\n",
    "    \"Visited Nodes\": visited_nodes_per_query,\n",
    "    \"Recall\": recall_per_query\n",
    "}).T\n",
    "results_df.index.name = \"Method\"\n",
    "results_df = results_df.reset_index()\n",
    "results_df = results_df.sort_values(by=[\"Method\"])\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:venv]",
   "language": "python",
   "name": "conda-env-venv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
